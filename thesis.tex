\documentclass[12pt]{report}

% Packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{color}
\definecolor{lightgray}{rgb}{0.95,0.95,0.95}
\lstnewenvironment{code}{\lstset{language=R, backgroundcolor=\color{lightgray},keywordstyle=\color{blue}, linewidth = \linewidth, xleftmargin= 1cm,xrightmargin= 1cm,breaklines= true,basicstyle=\scriptsize, frame=single,numbers=left, tabsize=12, literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\EUR}}1 {£}{{\pounds}}1}}{}





\usepackage{setspace}
\usepackage{lipsum}  % To generate dummy text for testing (remove in final version)
\usepackage{fontenc}  % For proper font encoding
\usepackage{times}    % Use Times New Roman font
\usepackage{geometry} % Adjust margins
\usepackage{pdfpages}
\usepackage[backend=biber,style=apa,sorting=nyt]{biblatex}
\usepackage{csquotes}  % Pour la gestion correcte des citations
\usepackage{hyperref}

\addbibresource{Impact of the prediction markets.bib}

% Page size and margins
\geometry{a4paper, left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

% Eviter les alinéas
\setlength{\parindent}{0pt}  % Désactive l'indentation de tous les paragraphes

% Line spacing
\renewcommand{\baselinestretch}{1.5}  % 1.5 line spacing

% Numérotation des sections sans chapitre
\setcounter{secnumdepth}{3} % Permet la numérotation jusqu'au sous-sous-section
\setcounter{tocdepth}{3}    % Inclure jusqu'à quel niveau dans la table des matières
\renewcommand{\thesection}{\arabic{section}} % Sections numérotées comme 1, 2, 3...
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}} % Sous-sections 1.1, 1.2...

% En-têtes et pieds de page
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage.} % Numéro de page en haut à droite
\renewcommand{\headrulewidth}{0pt} % Pas de ligne sous l'en-tête



\begin{document}


% Include the title page PDF
\includepdf[pages={1}]{Analyzing.pdf}


\chapter*{Abstract}


\chapter*{Acknowledgments}

I express my sincere gratitude to Delphia for the trust they have placed in me throughout this project. Their support has been invaluable and I am especially grateful for the data they provided which formed the backbone of my research.\\

I would also like to extend my heartfelt thanks to Professor Vande Kerckhove for his guidance, feedback and constant encouragement. His expertise has been instrumental in shaping this research and helping me navigate the complexities of this topic.\\

Finally, I would like to acknowledge all those who have contributed to this project, either directly or indirectly. Your support, insight and encouragement have been greatly appreciated. I am grateful to each one of you.\\



\tableofcontents

\newpage \section{Introduction}

In a rapidly evolving financial landscape, the role of information in shaping market behavior has become increasingly significant. One of the most intriguing and complex challenges is understanding the relationship between prediction markets and their influence on stock price movements. This thesis addresses the problem of quantifying how predictive signals derived from prediction markets impact stock prices.

\subsection{The Current World of Prediction Markets and Stock Prices}

In today’s financial world, markets are flooded with a wide range of information that can impact investor behavior (\cite{li_effect_2014}). Prediction markets, also known as event futures or information markets, aggregate individual opinions and probabilities about future events, offering valuable signals about the direction of various economic, political or social factors (\cite{waitz_corporate_2013}; \cite{wolfers_prediction_2004}). These markets provide a unique opportunity for investors to gauge future expectations and anticipate market movements.

\subsection{Research Question}

This thesis focuses on the research question: \textit{How do prediction markets signals affect stock prices?} To answer this, we explore the potential mechanisms through which these signals influence investor behavior and, by extension, the market valuation of assets. By investigating prediction markets signals, this research aims to build a comprehensive model for stock price prediction that accounts for the full spectrum of information available to investors.\\

To address this question, we adopt a comparative modeling strategy. Specifically, we assess the relevance and performance of a global modeling approach — a single regression model applied across all stocks, incorporating both returns and exogenous variables common to all assets — versus a stock-specific model, where a distinct model is trained for each individual stock. This methodology enables to determine whether a shared structure, enriched by exogenous information from prediction markets, can generalize across different stocks, or whether asset-specific nuances require tailored models.


\newpage
\subsection{Managerial Relevance}

This research holds substantial managerial relevance for Delphia, a Canadian fintech specializing in data-driven investment strategies. The implications are outlined below : \\

\textbf{1. Enhanced data-driven investment strategies:}  
By analyzing how predictive signals from prediction markets influence stock prices, Delphia can refine its investment models. Integrating these signals into algorithms could enhance decision-making processes and potentially improve portfolio performance (\cite{waitz_corporate_2013}).\\

\textbf{2. Improved forecasting accuracy:}  
Prediction markets aggregate public sentiment and probabilities about future events, offering valuable insights to anticipate market trends. By combining these signals with Delphia’s existing data models, the accuracy of stock price predictions could be significantly improved, leading to more informed investment decisions and better risk management.\\

\textbf{3. Detection of market inefficiencies and early trends:}  
This research can aid Delphia in identifying market inefficiencies and emerging trends by leveraging prediction market data. This proactive approach could allow the company to capitalize on shifts in market dynamics before competitors.\\

\textbf{4. Risk management optimization:}  
Incorporating prediction market signals would bolster Delphia’s ability to detect potential risks during periods of volatility or uncertainty. This enhanced risk management capability could help protect the company’s portfolios from significant losses and reduce exposure to high-risk investments.\\

\textbf{5. Innovation in investment products:}  
The findings could inspire the development of new financial products that utilize prediction market signals. For instance, funds tailored to exploit trends from these markets could offer clients innovative, high-return investment options.\\

\textbf{6. Competitive advantage through strategic information use:}  
Expanding Delphia’s data integration to include prediction markets could provide a distinct competitive edge. By harnessing diverse information sources, the company can position itself as a leader in identifying and acting on emerging trends, attracting clients and increasing its market presence.\\

Overall, this research aligns closely with Delphia’s mission of leveraging data to inform investment decisions. The insights gained can optimize strategies, improve predictive accuracy and strengthen the company’s position in the fintech sector.


\subsection{Thesis Structure}

This thesis is organized into chapters. The second chapter, Literature Review, examines the existing literature on prediction markets, stock price dynamics and recent advancements in global modeling applied to finance. This sets a comprehensive foundation for the research. In the third chapter, Methodology, the focus is on data collection methods, the techniques employed to implement global modeling and the integration of prediction markets to enhance the understanding of stock market movements. This chapter provides a detailed explanation of the methodological framework, including the implementation of prediction market data and  global modeling. Chapter four, Analysis of the results, presents the findings of the empirical analysis, showcasing how prediction markets influence stock prices and evaluating the effectiveness of global modeling techniques in interpreting these dynamics. The fifth chapter, Discussion, interprets the results, compares them with existing literature and discusses their implications for both theory and practice. Finally, the Conclusion summarizes the research, highlights its implications for investors and regulators and suggests areas for future research.
\\


\newpage
\section{Literature Review}
This section examines previous theories and research on the valuation of information and its influence on financial markets. It reviews key studies, theoretical frameworks and findings relevant to understanding the connection between prediction markets and stock market responses.



\subsection{Prediction Markets}

Prediction markets are platforms where participants trade contracts that pay out based on the outcome of future events (\cite{wolfers_prediction_2004}). These markets aggregate the individual predictions of participants, turning them into a collective forecast of the likelihood that an event will occur. The concept of prediction markets is rooted in the idea that the collective wisdom of a diverse group of individuals, each with access to different information and insights, can generate more accurate forecasts than any individual expert (\cite{bossaerts_price_2022}). These markets function similarly to financial markets (\cite{wolfers_prediction_2004}), where participants buy and sell contracts based on their predictions of future outcomes. For instance, participants might buy a contract that pays out if a specific political event occurs, such as the election of Trump against Kamala Harris during the 2024 presidential race (\cite{mongrain_introduction_2024}), or if a company meets a certain financial target.\\

One of the key features of prediction markets is their ability to aggregate information. As discussed by \cite{bossaerts_price_2022}, these markets incorporate diverse viewpoints, enabling them to reflect the collective intelligence of participants. This phenomenon is often referred to as the "wisdom of crowds". \cite{berg_prediction_2008}, have shown that prediction markets can outperform individual experts in terms of forecasting accuracy, as they incorporate a wide range of opinions. \\

These markets are typically structured as winner-take-all markets or vote-share markets. In winner-take-all markets (\cite{dai_wisdom_2021}), participants predict a single outcome and those who predict the correct event share the prize. This market model offers a fixed payout for the correct prediction, which is typically the same for all winners. Vote-share markets involve trading based on the estimated proportion of votes or shares in a specific outcome, often used for more detailed or continuous predictions, such as estimating the exact vote share in an election or market share in a competition (\cite{dai_wisdom_2021}).\\

Research has shown that prediction markets can effectively predict not only political events but also economic trends, such as the likelihood of a recession or the movement of stock prices (\cite{wolfers_prediction_2004}). These markets function as a tool to forecast economic indicators, company performance, or even geopolitical events. By integrating real-time information and the collective insights of a diverse set of participants, prediction markets offer a unique and valuable source of predictive signals that can inform decision-making in various domains, including finance. For example, Google estimates its market capitalization using prediction markets to forecast its value prior to an initial public offering (\cite{berg_searching_2009}).


\subsection{Conceptual and theoretical framework}

\subsubsection{Efficient Market Hypothesis (EMH)}

The Efficient Market Hypothesis (EMH), developed by Eugene Fama, posits that asset prices fully and instantaneously reflect all available information (\cite{naseer_efficient_2016}). This theory assumes that markets are highly efficient in processing and integrating information into prices, leaving no opportunity for consistent outperformance based on publicly available data.\\

In this context, prediction markets can be viewed as a mechanism to enhance market efficiency by aggregating dispersed and heterogeneous information from a diverse set of participants (\cite{downey_efficient_2024}). These markets function by incentivizing participants to trade based on their knowledge, insights, or expectations of future outcomes, effectively pooling collective intelligence.\\

Unlike traditional financial markets, which rely on a combination of fundamental and technical analysis, prediction markets are explicitly designed to reflect probabilities of future events (\cite{nti_systematic_2020}; \cite{naseer_efficient_2016}). By doing so, they may uncover "hidden" information that would otherwise remain unintegrated into asset prices. This ability to aggregate a wide array of viewpoints and data sources positions prediction markets as a complementary tool for improving the informational efficiency of financial markets.

\subsubsection{Investor behavior}

\paragraph{Herding :}Herding behavior is a significant phenomenon in financial markets, where investors mimic the actions of others rather than relying on their own private information. This tendency often arises during periods of market stress, leading to deviations of asset prices from their intrinsic values. Research highlights that herding is particularly pronounced in emerging markets, where information asymmetry and market inefficiencies are more prevalent (\cite{ah_mand_herding_2023}). For example, in the Malaysian stock market, evidence suggests that herding behavior exhibits non-linear characteristics, with variations between up and down market conditions. Shariah-compliant stocks tend to demonstrate herding more significantly during upward market movements, while conventional stocks show limited evidence of herding. This behavior has critical implications for market stability, as it may amplify volatility and hinder market efficiency.

\paragraph{Financial influencers :}Financial influencers on social media platforms, particularly those categorized as "mega influencers" with over one million followers, have a unique ability to shape investor behavior and market dynamics. Studies demonstrate that posts from these influencers can significantly affect investor attention, trading volume and stock price volatility (\cite{keasey_impact_2024}). However, their influence on stock returns is limited, requiring posts with extreme sentiment from top influencers to elicit short-lived impacts on returns. This aligns with the "noise trader" hypothesis, which posits that uninformed trading triggered by such influencers introduces temporary mispricing that reverses over time. By analyzing over 16 million Instagram posts, researchers have highlighted the importance of sentiment and engagement metrics, such as comment volume, in amplifying the visibility and potential market impact of influencer content. These findings underscore the dual-edged role of influencers in promoting market participation while potentially fostering instability through noise trading.


\newpage
\section{Methodological Framework}
This section outlines the conceptual and theoretical underpinnings of the research. It begins with a comparative modeling strategy employed, contrasting global versus local models, and explores the theoretical principles, such as the bias-variance tradeoff, guiding this choice. Subsequently, it articulates the testable hypotheses that stem from this framework. The research design is then detailed, covering the overall experimental architecture, the rationale for employing temporal validation methods like walk-forward analysis with specific parameters used in the implementation, and the considerations for its practical execution. Finally, it describes the evaluation methodology, including the selection of Root Mean Squared Error (RMSE) as the primary performance metric and the use of the Diebold-Mariano test for comparing forecasting model efficacy, as implemented in the
analysis.

\subsection{Comparative Modeling Strategy: Global vs. Local Approaches}

In financial time series forecasting, one crucial methodological decision involves the level of aggregation in model training — whether to use a single unified model across multiple assets (global modeling) or to estimate distinct models for each asset (local modeling). This distinction has important implications for model complexity, data efficiency, and forecasting accuracy. The present research implements and compares both approaches in the context of predicting daily stock returns using signals from prediction markets as exogenous inputs.

\subsubsection{Global Models}

A global model is trained using the pooled data from all stocks in the dataset. In this study, a tree-based ensemble algorithm — specifically XGBoost — was used for global modeling due to its ability to handle high-dimensional, heterogeneous features and capture non-linear interactions. The premise of global modeling lies in the assumption that underlying drivers of stock returns share common structures across assets — such as macroeconomic shocks, investor sentiment, or systemic risk factors — that can be learned jointly (\cite{hartford_deep_2018}; \cite{gu_empirical_2020}).\\

By training a model across all series simultaneously, the global approach benefits from a larger effective sample size, which typically reduces variance and improves generalizability (\cite{montero_modelling_2021}). Furthermore, the presence of exogenous variables — such as aggregated sentiment from prediction markets — reinforces the potential effectiveness of a global structure by exploiting shared informational signals. However, global models may suffer from specification bias if individual assets exhibit idiosyncratic behaviors that cannot be adequately captured by a single unified function.

\subsubsection{Local Models}

In contrast, local models are estimated separately for each stock. Each asset is modeled independently, allowing the estimation process to capture its unique autoregressive patterns and responsiveness to prediction market signals. For this study, we implemented ARIMA and ARIMAX models, which are well-suited to univariate or low-dimensional time series data with clear temporal dependencies. The local approach assumes that stocks respond to signals in a firm-specific manner, which may reflect structural differences in sectors, liquidity, or investor composition (\cite{de_prado_advances_2018}; \cite{tashman_out-of-sample_2000}).\\

Local modeling allows for lower bias by fitting asset-specific dynamics; however, it comes at the cost of higher estimation variance, especially when limited historical data is available per asset. This tradeoff becomes particularly acute in short time series, where overfitting and parameter instability can degrade forecasting performance. From a practical standpoint, maintaining and updating hundreds of separate models also poses operational complexity, especially in high-frequency or live trading environments.

\subsubsection{Bias-Variance Tradeoff and Model Evaluation}

This dual modeling approach is grounded in the classical bias-variance tradeoff (\cite{geman_bias_1992}). Global models, by pooling data, offer stable predictions with lower variance but may introduce systematic bias if heterogeneity across series is substantial. Local models are more flexible and tailored, potentially reducing bias, but are more prone to variance-driven errors. This empirical analysis evaluates these tradeoffs in the context of out-of-sample forecasting error, using Root Mean Squared Error (RMSE) and the Diebold-Mariano test to compare model performance statistically across assets and time periods.


\subsection{Testable Hypotheses Derived from the Framework}

The following hypotheses guided the empirical investigation:
\begin{itemize}
    \item \textbf{H1:} Prediction markets provide significant incremental information for stock return forecasting. This posits that signals from prediction markets, when incorporated into forecasting models, improve predictive accuracy compared to models using only historical data.

    % H1 (fr) : Les marchés de prédiction apportent une information supplémentaire significative pour la prévision des rendements boursiers.

    \item \textbf{H2:} The informational value of prediction market signals for stock return forecasting is more pronounced in local (stock-specific) models than in global (market-wide) models. This explores whether local models better leverage granular prediction market information.

    % H2 (fr) : La valeur informationnelle des signaux des marchés de prédiction est plus marquée dans les modèles locaux que dans les modèles globaux.

    \item \textbf{H3:} The nature and strength of the relationship between prediction market signals and stock returns exhibit heterogeneity across different stocks. This acknowledges that the impact of prediction market signals may vary by factors like sector or company characteristics (though the implemented code focuses on model comparison rather than deep analysis of this heterogeneity for H3).

    % H3 (fr) : La nature et l’intensité de la relation entre les signaux issus des marchés de prédiction et les rendements boursiers varient selon les actions.
\end{itemize}

\subsection{Research Design}
The research design delineated the systematic approach to empirically test the hypotheses, ensuring robustness in evaluating model performance.

\subsubsection{Overall Experimental Structure}

The experiment assessed the forecasting accuracy of global and local models for each individual stock. Two modeling paradigms were compared:

\begin{itemize}
    \item \textbf{Global model:} A single XGBoost model was trained using features including prediction market signals to forecast returns for all stocks collectively. This model captures potential cross-stock patterns and global market dynamics.
    
    \item \textbf{Local models:} A distinct ARIMA-based model (ARIMA or ARIMAX) was built for each stock. For ARIMAX models, prediction market signals were included as exogenous regressors, but only if they exhibited Granger causality with the target stock (i.e., if they were shown to provide predictive power over past values of the stock in a time series sense). If no prediction market signals passed the Granger test for a given stock, a standard ARIMA model was used instead.
\end{itemize}

The comparison was made on a per-stock basis: for each stock, the performance of its local model was compared against the global model’s forecast for that same stock. This design allowed a direct evaluation of whether stock-specific (local) models leveraging causally relevant prediction signals outperformed a global, feature-rich model.

Performance was evaluated on an out-of-sample period using walk-forward validation to ensure temporal robustness.


\subsubsection{Temporal Validation (Walk-Forward)}

Given the temporal nature of financial time series, a walk-forward validation approach was implemented using the \texttt{time\_series\_cv} function from the \texttt{timetk} package in R. This method preserves the chronological order of observations and simulates real-world forecasting scenarios, where models are periodically re-estimated as new data becomes available. Such an approach is essential to avoid look-ahead bias and to assess model stability and adaptability in dynamic financial markets.

\subsubsection{Number and Duration of Folds}

Walk-forward validation was configured with the following parameters for the \texttt{time\_series\_cv} function:
\begin{itemize}
    \item \texttt{initial = "150 days"}: approximately 5 months of data used for the initial training set.
    \item \texttt{assess = "75 days"}: approximately 2.5 months allocated for each test (assessment) period.
    \item \texttt{skip = "75 days"}: the window advances by 2.5 months after each assessment period.
    \item \texttt{cumulative = TRUE}: the training window expands cumulatively, incorporating previous assessment data.
\end{itemize}


\subsection{Evaluation Methodology}
The evaluation methodology provided a comprehensive and statistically sound assessment of model predictive performance.

\subsubsection{Choice of RMSE as the Primary Metric}

The Root Mean Squared Error (RMSE) was chosen as the primary metric for evaluating forecast accuracy. RMSE penalizes larger errors more heavily and is expressed in the same units as the target variable (stock returns). It was computed using the \texttt{modeltime\_accuracy} function from the \texttt{modeltime} package in R.

\subsubsection{Choice of the Diebold-Mariano Test}

To statistically compare the predictive accuracy of two competing forecasting models (e.g., global vs. local models), the Diebold-Mariano (DM) test was employed using the \texttt{dm.test} function from the \texttt{forecast} R package. The null hypothesis of the test is that both models have equal forecast accuracy. The test was conducted on the forecast errors (denoted as \( e1_t \)).\\

The \texttt{h} parameter (forecast horizon) for the DM test was defined as \texttt{max(1, min(1, floor(n\_obs / 2)))}, where \( n\_obs \) represents the number of common observations between the two error series. This heuristic is commonly used when a specific forecast horizon is not pre-defined or is expected to be short. The test was performed using a two-sided alternative hypothesis (\texttt{alternative = "two.sided"}).

\subsubsection{Criteria for Interpreting Results}

Interpretation of the results relied on RMSE values and the statistical significance (p-values) obtained from the Diebold-Mariano tests. A p-value below 0.05 was considered indicative of a statistically significant difference in forecast accuracy. The results were presented using formatted tables generated with the \texttt{kable} and \texttt{kableExtra} packages in R, which included conditional formatting to highlight the better-performing model for each stock and validation fold. Summary statistics, including the frequency with which local or global models performed better and the average improvement, were also provided.



\newpage
\section{Data and Preprocessing}

This section describes the data sources and the preprocessing steps applied to prepare the datasets for empirical modeling, distinguishing between the procedures used for the global and local modeling strategies. The implementation was done in \texttt{R}, utilizing a wide range of packages including \texttt{tidymodels}, \texttt{modeltime}, \texttt{timetk}, \texttt{tidyverse}, \texttt{lubridate}, \texttt{KFAS}, \texttt{tseries}, \texttt{urca}, and others.

\subsection{Data Sources}
The empirical analysis is based on three primary categories of data, each serving a complementary purpose in the modeling and evaluation process:

\begin{itemize}
    \item \textbf{Prediction Market Data (Kalshi):} Forecast data were collected from Kalshi, a regulated prediction market platform, covering a wide range of macroeconomic and firm-specific events. These included both quarterly contracts (e.g., Tesla vehicle production, Netflix subscriber growth, Meta daily active users, U.S. GDP growth) and annual contracts (e.g., hurricane counts, SpaceX launches, tech layoffs, Apple car announcements, Ethereum price, measles incidence). Data was provided in CSV format (e.g., \texttt{kalshi-chart-data-tesla-24-q1.csv}), with time-stamped forecast values or percentages. Custom parsing functions in \texttt{R} were used to standardize and convert these values to a daily frequency suitable for time series modeling.

    \item \textbf{Financial Market Data (Delphia):} Stock return data for a curated selection of U.S. equities were provided by Delphia, a data-driven fintech company. These returns, compiled in the file \texttt{tilt\_stocks\_2024.csv}, include daily observations for prominent stocks such as TSLA, NFLX, META, AAPL, NVDA, MSFT, and others. This dataset served as the primary dependent variable for evaluating the predictive power of Kalshi-based signals on individual company returns.

    \item \textbf{ETF Market Data (Yahoo Finance via \texttt{yfinance}):} To investigate whether the relationships observed between Kalshi signals and individual stock returns also extend to broader asset classes, exchange-traded funds (ETFs) were incorporated into the analysis. A Python script leveraging the \texttt{yfinance} library was used to query and download historical price and return data for four major ETFs:
    \begin{itemize}
        \item CAC 40 (\texttt{CAC.PA})
        \item S\&P 500 (\texttt{SPY})
        \item MSCI World Index (\texttt{IWDA.AS})
        \item Nasdaq 100 (\texttt{QQQ})
    \end{itemize}
    These ETFs were chosen to represent a range of geographies and sector exposures, allowing the study to explore whether insights derived from Kalshi signals are consistent across both equity-specific and index-based instruments.
\end{itemize}


\subsection{Common Preprocessing Pipeline}
For both the global and local models, the following preprocessing steps were systematically applied:

\begin{itemize}
    \item \textbf{Daily Aggregation:} Raw Kalshi prediction market data came with heterogeneous temporal structures. Some contracts were issued on a quarterly basis (e.g., Tesla Q1–Q4 production), others annually (e.g., total hurricanes in the year), and certain signals were updated multiple times per day. To ensure compatibility with daily financial returns, all prediction signals were aggregated to a daily frequency using a custom function (\texttt{process\_df\_daily}). This function computed the mean of all intraday values for each contract and date, producing one representative prediction value per day per event.

    \item \textbf{Missing Value Imputation:} The handling of missing data differed depending on the data type:
    \begin{itemize}
        \item \textit{Prediction Market Data:} Between prediction periods (e.g., from Q1 to Q2), some dates were missing due to inactive or unpublished forecasts. To reconstruct a complete daily timeline for each contract, a Kalman filter was applied using a local linear trend state-space model (\texttt{SSModel}) from the \texttt{KFAS} package. The smoothed state estimates filled in the missing values, ensuring temporal continuity across contract boundaries.
        
        \item \textit{Financial Market Data:} Stock and ETF return data naturally omitted weekends and public holidays, during which financial markets are closed. These are not true data gaps but expected non-trading days. As such, no imputation was performed. The data was simply filtered to exclude these dates using \texttt{na.omit()}, ensuring that models were trained only on valid trading sessions (typically five days per week).
    \end{itemize}

    \item \textbf{Data Integration:} Once the prediction signals and financial return series were processed, the two datasets were merged on the basis of the available trading dates from the financial market data (i.e., stock and ETF returns). This approach ensured temporal alignment and eliminated any potential look-ahead bias, by guaranteeing that only information available at or prior to each trading day was used for forecasting. By anchoring the integration to the financial data timeline, we avoided including future prediction signals that would not have been observable at the time of the actual market movement. The final dataset consisted of: (i) daily asset returns as target variables, (ii) corresponding prediction market signals, and (iii) metadata such as date, asset ticker, and event identifiers.
\end{itemize}

To reinforce temporal causality and prevent future information from influencing model training or evaluation, the integration pipeline was anchored on financial market timestamps, ensuring that only contemporaneous or past Kalshi signals were matched with asset returns. In particular:
\begin{itemize}
    \item Only prediction signals published prior to or on the same day as the asset return were used.
    \item Future data—even if technically available in the full dataset—was never leaked into the training or testing process.
    \item All model evaluations were conducted using walk-forward cross-validation, with strict temporal separation between training and test sets.
\end{itemize}

This careful design eliminated the risk of contaminating the model with information from the future and ensured the empirical validity of the forecasting exercise.

\newpage
\subsection{Global Modeling Preprocessing}
In the global modeling framework (\texttt{global\_model.R}), the goal was to identify generalizable relationships between Kalshi forecast signals and financial asset returns by training a single unified model across all stocks. Unlike the individual asset approach, a common model architecture was used to highlight predictive features that are valid across companies, contracts, and time periods.

\begin{itemize}
    \item \textbf{Data Structure:} After preprocessing and integration steps, the forecast signals and financial returns were merged into a longitudinal dataset (long format), covering all tickers. Each row represented a unique combination of trading date and stock ticker, with Kalshi signals as explanatory variables and the daily return as the target variable.

    \item \textbf{Feature Engineering:} Kalshi signals were converted from long format to wide format using the \texttt{pivot\_wider()} function. Each distinct Kalshi contract (e.g., \texttt{TSLA\_Q1\_prod}, \texttt{META\_DAU}) became a separate explanatory variable column. This transformation led to a high-dimensional feature space. Missing values—often due to contracts being specific to certain companies—were kept and handled directly by the model.

    \item \textbf{Metadata Addition:} In addition to the forecast signals, categorical variables such as the stock ticker (\texttt{ticker}) were included as predictors. These were encoded into dummy variables using \texttt{step\_dummy()} from the \texttt{recipes} package. This allowed the model to learn company-specific effects while maintaining a common modeling structure.

    \item \textbf{Temporal Split and Causal Integrity:} The data were sorted chronologically and then split into training and test sets to ensure that the test periods were distinct and subsequent to the training sets. This method preserved the temporal integrity of the data, preventing any future information from contaminating the training phase. All preprocessing steps—including imputation, encoding, and normalization—were applied only to the training sets, maintaining a strict separation between phases.

    \item \textbf{Handling Missing Values:} The XGBoost algorithm has native handling of missing values, which eliminated the need for additional imputation. Missing values in Kalshi signals—usually due to contracts not relevant for certain companies—were left as \texttt{NA} and passed directly to the model. This allowed the model to use the absence of information as a signal in itself.
\end{itemize}

This global approach enabled the exploitation of inter-company variation while maintaining temporal rigor. By training on a unified dataset covering various stocks and types of contracts, the model aimed to capture robust and generalizable latent structures, thus improving predictive performance and transferability.

\subsection{Local Modeling Preprocessing}
While the global model adopted a unified architecture across assets, the local modeling approach (\texttt{local\_model.R}) focused on constructing individualized models for each stock, allowing for asset-specific dynamics to be captured more effectively. This section details the additional preprocessing steps and methodological refinements applied in this localized framework.

\begin{itemize}
    \item \textbf{Asset-Specific Subsetting:} For each asset (e.g., TSLA, NFLX, META), the dataset was filtered to isolate the corresponding return series and its associated Kalshi contracts. This ensured that only relevant signals were considered, reducing noise and dimensionality for each model instance.

    \item \textbf{Stationarity Assessment and Differencing:} 
    As local models were more sensitive to temporal properties of the time series, both the return and prediction signal series were subjected to stationarity diagnostics. A custom function (\texttt{check\_stationarity}) applied the Augmented Dickey-Fuller (ADF) test recursively, with up to two orders of differencing if required. This ensured that the modeling inputs met the assumptions of stationarity-critical techniques, such as vector autoregression and Granger causality.

    \item \textbf{Dynamic Predictor Selection via Granger Causality:}
    To identify which Kalshi contracts contained statistically significant predictive information for a given asset, pairwise Granger causality tests were conducted between the differenced prediction signals and the asset’s return series. Only predictors passing the causality threshold (typically at a 5\% significance level) were retained. This procedure filtered out noisy or irrelevant features, yielding a parsimonious and asset-specific feature set for each local model.

    \item \textbf{Customized Feature Engineering:}
    Unlike the global model, no dummy variables for tickers or cross-sectional metadata were included, as each local model was inherently tied to a specific stock. Instead, lagged features were created on a per-asset basis using functions from the \texttt{timetk} and \texttt{recipes} packages, allowing the model to incorporate short-term memory effects and autoregressive structures.

    \item \textbf{Residual Analysis and Iterative Refinement:}
    After initial model training (e.g., with XGBoost or linear regression), residual diagnostics were performed. This included checks for autocorrelation (e.g., using ACF plots), heteroscedasticity, and structural breaks. When inadequacies were detected, models were refined iteratively—either by adjusting the lag structure, reconsidering differencing depth, or revising the predictor set.

    \item \textbf{Temporal Cross-Validation:}
    As with the global model, temporal integrity was strictly enforced. Walk-forward cross-validation was applied individually to each asset-specific dataset, ensuring that the validation periods always followed the training sets in time. Preprocessing steps—including stationarity transformation and causality testing—were nested within each fold to avoid information leakage.
\end{itemize}

This tailored approach enabled a deeper exploration of asset-specific dynamics and improved modeling accuracy in cases where global signals might dilute or overlook localized patterns. While computationally more intensive, the local strategy allowed for greater interpretability and robustness in the presence of heterogeneous signal relevance across assets.

\newpage
\section{Modeling (Implementation)}
This section details the practical implementation of the forecasting models. The primary global model implemented was XGBoost, and local models were ARIMA, both evaluated within a walk-forward validation framework.

\subsection{Technical specifications of the global model (XGBoost)}

The global forecasting model used in this project is based on the XGBoost algorithm, a powerful gradient boosting technique particularly well-suited for structured/tabular data. The model is trained and evaluated using a time-series cross-validation framework with walk-forward validation to preserve the temporal structure of the data. Below are the key steps of the implementation:

\paragraph{Data Splitting and Cross-Validation:}
To evaluate the model's generalization ability, we use the \texttt{time\_series\_cv()} function to implement a walk-forward validation strategy. This method ensures that each training set contains only observations prior to the corresponding testing period. We use two non-overlapping folds with the following configuration:\\

\begin{code}[language=R]
# Crée deux splits temporels non superposés avec fenêtre cumulative
splits <- data_tbl %>%
  arrange(date) %>%
  time_series_cv(
    date_var   = date,        # Variable de temps pour organiser les observations
    initial    = "150 days",  # Fenêtre d'initialisation: 150 jours de données pour l'entraînement
    assess     = "75 days",   # Fenêtre d'évaluation: 75 jours de données pour le test
    skip       = "75 days",   # Décalage de 75 jours entre chaque fold
    cumulative = TRUE,        # Cumulatif: chaque fenêtre d'entraînement inclut toutes les données précédentes
    slice_limit = 2           # Limitation à 2 folds pour la validation
  )
\end{code}\\

In this section of the code, we create a walk-forward validation strategy using the \texttt{time\_series\_cv()} function. First, we ensure that the data is ordered by date, which is essential for time-series analysis. The function splits the data into two folds with a training window of 150 days and a testing window of 75 days. The training window is cumulative, meaning each new fold includes all previous data. The \texttt{skip} argument is set to 75 days, meaning the training set for the next fold will start 75 days after the previous fold's test set. Finally, the \texttt{slice\_limit} argument is used to limit the number of folds to 2 for simplicity.

\paragraph{Feature Engineering:}
Each training fold is processed through a \texttt{recipe} object, which includes the following transformations:

\begin{itemize}
    \item Conversion of categorical features to dummy variables using \texttt{step\_dummy()}
    \item Normalization of numeric predictors to have zero mean and unit variance using \texttt{step\_normalize()}
    \item Extraction of time-based features using \texttt{step\_timeseries\_signature()}
    \item Removal of near-zero variance predictors using \texttt{step\_zv()}
\end{itemize}

\begin{code}
# Pipeline de préparation des données pour XGBoost
rec_obj_xgb <- recipe(value ~ ., data = train_data) %>%
  step_dummy(id) %>%  # Conversion des variables catégorielles en variables indicatrices (dummy)
  step_normalize(all_numeric(), -all_outcomes()) %>%  # Normalisation des prédicteurs numériques (sauf la variable cible)
  step_timeseries_signature(date) %>%  # Extraction des caractéristiques temporelles à partir de la variable "date"
  step_rm(date) %>%  # Suppression de la variable "date" car elle n'est plus nécessaire après l'extraction des caractéristiques temporelles
  step_zv(all_predictors()) %>%  # Suppression des prédicteurs avec une variance proche de zéro
  step_dummy(all_nominal_predictors(), one_hot = TRUE)  # Conversion des prédicteurs nominaux en variables "one-hot"
\end{code}\\

The \texttt{recipe} function is used to preprocess the data for the XGBoost model. This includes several important steps. First, categorical variables (such as "id") are converted into dummy variables using \texttt{step\_dummy()}. Numeric predictors are then normalized to have zero mean and unit variance using \texttt{step\_normalize()}. Time-based features are extracted with\\
\texttt{step\_timeseries\_signature()} to allow the model to learn seasonal patterns. The \texttt{step\_rm()} function removes the "date" column after the time-based features are extracted. Next, \texttt{step\_zv()} removes predictors that have near-zero variance, which are unlikely to contribute to the model. Finally, \texttt{step\_dummy()} is applied to nominal predictors to perform one-hot encoding, which transforms categorical variables into a format suitable for machine learning algorithms.\\

\paragraph{Model Training:}
For each fold, an XGBoost regression model is trained within a \texttt{workflow} using the previously defined recipe. This workflow combines the data preprocessing steps with the model training.\\

\begin{code}
# Création d’un workflow combinant le modèle XGBoost avec la recette
wflw_xgb <- workflow() %>%
  add_model(boost_tree("regression") %>% 
  set_engine("xgboost")) %>%  # Spécification de l'algorithme XGBoost pour la régression
  add_recipe(rec_obj_xgb) %>%  # Ajout de la recette de prétraitement
  fit(train_data)  # Entraînement du modèle sur les données d'entraînement
\end{code}\\

In this part of the code, an XGBoost model is created using the \texttt{workflow()} function. This workflow integrates the pre-processing steps from the recipe with the XGBoost model. The model is specified as a regression model by using the \texttt{boost\_tree()} function and setting the engine to \texttt{xgboost}. The \texttt{add\_recipe()} function adds the preprocessing steps defined earlier, ensuring that the data is properly prepared before fitting the model. Finally, the model is trained using the \texttt{fit()} function on the training data.

\paragraph{Model Evaluation:}
The model is evaluated using the \texttt{modeltime} framework. For each fold, the calibrated model is assessed on unseen data and its accuracy is reported both globally and by ticker (i.e., for each time series). RMSE is used as the main metric to assess model performance.\\

\begin{code}
# Calibrage du modèle sur les données de test
calib_tbl <- model_tbl %>%
  modeltime_calibrate(new_data = test_data, id = "ticker")  # Calibrage sur les données de test, par ticker

# Calcul des performances (RMSE, etc.) par ticker (id)
calib_tbl %>%
  modeltime_accuracy(acc_by_id = TRUE)  # Calcul des métriques de performance par ticker
\end{code}\\

Once the model is trained, it is calibrated on the test data using the \texttt{modeltime\_calibrate()} function. This function evaluates the model's performance on unseen data, and accuracy metrics (like RMSE) are calculated using the \texttt{modeltime\_accuracy()} function. The evaluation is done both globally and by each ticker to assess the model's performance on individual time series.

\paragraph{Forecast Error Computation:}
Forecast errors \( e_t = y_t - \hat{y}_t \) are computed and stored for further evaluation, combining both folds. This provides a comprehensive understanding of model performance over time, helping to identify patterns in prediction errors.

\paragraph{Final Forecasting and Refit:}
After validation, the model is refitted on the full dataset to produce future forecasts. A 30-day future frame is generated for each ticker, and forecasts with confidence intervals are produced.\\

\begin{code}
# Réentraînement du modèle sur l'ensemble des données (après validation)
refit_tbl <- calib_tbl_1 %>%
  modeltime_refit(data = data_tbl)  # Réentraînement du modèle sur l'ensemble des données disponibles

# Génération des prévisions sur 30 jours pour chaque ticker
forecast_results <- refit_tbl %>%
  modeltime_forecast(
    new_data = future_tbl,  # Données futures pour lesquelles générer les prévisions
    actual_data = data_tbl,  # Données réelles historiques pour calculer les intervalles de confiance
    conf_by_id = TRUE  # Calcul des intervalles de confiance par ticker
  )
\end{code}

After model validation, the final model is refitted using the entire dataset with \texttt{modeltime\_refit()}. This allows the model to incorporate all available data, including the validation data, before making future predictions. A 30-day future frame is generated for each ticker, and predictions are made using the \texttt{modeltime\_forecast()} function, which also calculates confidence intervals for the forecasts.

\paragraph{Result Storage:}
Both RMSE metrics and final forecast outputs are saved as RDS files for reproducibility and further analysis. This ensures that results can be easily accessed and shared for future analysis or reporting.\\

\begin{code}
# Sauvegarde des résultats de RMSE et des prévisions
saveRDS(global_rmse_results, "data/rds/global_model_rmse.rds")
# Sauvegarde des résultats RMSE pour chaque ticker
saveRDS(forecast_results, "data/rds/global_model_forecasts.rds")
# Sauvegarde des résultats de prévisions
\end{code}\\

The results, including RMSE metrics and forecast outcomes, are saved in RDS files using the \texttt{saveRDS()} function. This ensures that the model's results can be reproduced in the future and shared for further analysis or reporting.


\subsection{Technical specifications of the local model (ARIMAX)}
To study the relationship between stock returns and prediction markets, we have implemented a local modeling approach using ARIMAX, which is an ARIMA model incorporating external explanatory variables. The approach is structured according to the following steps:

\subsubsection{Checking the Stationarity of Stock Time Series}

Before conducting any time series analysis, it is essential to ensure that the series being analyzed are stationary. To do this, we apply the Augmented Dickey-Fuller (ADF) unit root test. The following code illustrates the procedure applied to each ticker:\\

\begin{code}[language=R,caption={Stationarity test with successive differencing}]
check_stationarity <- function(df, ticker) {
  df_clean <- df[!is.na(df$returns), ]
  adf_result <- adf.test(df_clean$returns, alternative = "stationary")
  
  if (adf_result\$p.value > 0.05) {
    df\$returns <- c(NA, diff(df\$returns, differences = 1))
    df_clean <- df[!is.na(df\$returns), ]
    adf_result_diff1 <- adf.test(df_clean\$returns, alternative = "stationary")
    
    if (adf_result_diff1\$p.value > 0.05) {
      df\$returns <- c(NA, diff(df\$returns, differences = 1))
    }
  }
  return(df)
}
\end{code}

\subsubsection{Cross-Correlation Analysis}

The objective is to examine temporal relationships between stock return series and prediction market series. To achieve this, we use the cross-correlation function (CCF), which helps detect lagged effects between the two series:\\\\

\begin{code}[language=R,caption={Cross-correlation computation between prediction markets and stocks}]
compute_cross_correlation <- function(stock_series, pred_series, max_lag = 20) {
  df_combined <- inner_join(stock_series, pred_series, by = "date", suffix = c("_stock", "_pred"))
  df_clean <- df_combined %>% drop_na(pred_daily_stock, pred_daily_pred)
  ccf_result <- ccf(df_clean$pred_daily_stock, df_clean$pred_daily_pred, lag.max = max_lag, plot = FALSE)
  
  result_df <- data.frame(
    lag = ccf_result\$lag,
    correlation = ccf_result\$acf,
    stock = unique(stock_series\$ticker)[1],
    prediction_market = unique(pred_series\$id)[1]
  )
  return(result_df)
}
\end{code}

\subsubsection{Time Splitting: Walk-forward CV}

We used a \textit{walk-forward cross-validation} time splitting method to simulate a real forecasting situation. The dataset is divided into successive windows, with a training period of 150 days and a test period of 75 days. This procedure is repeated for two folds:

\begin{itemize}
    \item \textbf{Fold 1:} Training data for the first 150 days, testing on the following 75 days.
    \item \textbf{Fold 2:} Cumulative training data (225 days), testing on the following 75 days.
\end{itemize}

\begin{code}
splits <- df_combined %>%
  arrange(date) %>%
  time_series_cv(
    date_var   = date,
    initial    = "150 days",
    assess     = "75 days",
    skip       = "75 days",
    cumulative = TRUE,
    slice_limit = 2
  )
\end{code}

\begin{code}
first_split <- splits\$splits[[1]]
second_split <- splits\$splits[[2]]

train_data_1 <- analysis(first_split)
test_data_1  <- assessment(first_split)
train_data_2 <- analysis(second_split)
test_data_2  <- assessment(second_split)
\end{code}


\subsubsection{Selection of Exogenous Variables}

For each stock, we apply a Granger causality test to identify which series from the prediction markets are significantly predictive. Only variables with a p-value below 5\% and results indicating that they \textit{cause} returns are retained as exogenous candidates.\\


\begin{code}[language=R,caption={Test de causalité de Granger entre les séries}]
granger_causality_test <- function(pred_series, stock_series, max_lag = 5) {
  df_combined <- inner_join(pred_series, stock_series, by = "date", suffix = c("_pred", "_stock"))
  df_clean <- df_combined %>% drop_na(pred_daily_pred, pred_daily_stock)
  grangertest(df_clean\$pred_daily_stock ~ df_clean\$pred_daily_pred, order = max_lag)
}
\end{code}

\subsubsection{Fitting the ARIMAX Model}

Based on the results of the Granger test, two scenarios are considered:
\begin{itemize}
    \item \textbf{No significant exogenous variables:} A simple ARIMA model is fitted.
    \item \textbf{At least one exogenous variable:} An ARIMAX model is fitted with the corresponding predictions as explanatory variables.
\end{itemize}

The exogenous variables are time-aligned with the return series and formatted into a matrix. Additional checks are performed to handle missing values and potential misalignments. In case of an error during fitting, the model defaults to a simple ARIMA model.\\

\begin{code}[caption={Modélisation locale ARIMA/ARIMAX pour chaque action}]
fit_local_model_fold <- function(ticker_data, granger_results, prediction_markets, fold_number) {
  # Extraction du titre courant
  current_ticker <- unique(ticker_data\$ticker)
  
  # Extraction des variables exogènes significatives
  exog_vars <- granger_results %>%
    filter(Stock == current_ticker, result == "Granger-causes", p_value < 0.05) %>%
    pull(Prediction_Market)

  ts_data <- ts(ticker_data\$returns, frequency = 365)

  if (length(exog_vars) > 0) {
    exog_matrix <- prediction_markets %>%
      filter(id %in% exog_vars) %>%
      pivot_wider(names_from = id, values_from = pred_daily) %>%
      arrange(date) %>%
      filter(date %in% ticker_data$date) %>%
      select(-date) %>%
      as.matrix()

    if (any(is.na(exog_matrix))) {
      model <- auto.arima(ts_data)
    } else {
      model <- auto.arima(ts_data, xreg = exog_matrix)
    }
  } else {
    model <- auto.arima(ts_data)
  }

  return(model)
}
\end{code}

\subsubsection{Evaluation via Recursive Forecasting (Rolling Forecast)}

For each fitted model, the performance is evaluated by generating the forecasts recursively to avoid any look-ahead bias:
\begin{itemize}
    \item Forecasts are made point by point, updating the model with the most recent available data.
    \item At each iteration, the values of the exogenous variables for the target date are used.
\end{itemize}

The performance criterion chosen is the root mean square error (RMSE) over the test period.

\subsubsection{Residual Analysis}

The residuals of the models are analyzed across several dimensions:
\begin{itemize}
    \item \textbf{Normality:} Shapiro-Wilk test.
    \item \textbf{Autocorrelation:} Ljung-Box test.
    \item \textbf{Heteroscedasticity:} Breusch-Pagan test.
    \item \textbf{Basic Statistics:} mean, standard deviation, skewness, and kurtosis.
\end{itemize}

These tests allow for assessing the quality of the fitted model and the validity of its assumptions.






\newpage
\section{Analysis of Results}





\newpage
\section{Discussion}
This section compares the results obtained with those of previous studies and outlines the practical and theoretical implications of the study. It also explores potential limitations and alternative
interpretations of the findings.

\newpage
\section{Conclusion}
This final section summarizes the main findings of the study, discusses its limitations and suggests avenues for future research.


\newpage
\printbibliography

\newpage
\chapter*{Appendix}

% Include the title page PDF
\includepdf[pages={1}]{end_page.pdf}


\end{document}

