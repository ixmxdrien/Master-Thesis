# Check for informative variables
if (ncol(exog_matrix) == 0 || all(apply(exog_matrix, 2, function(x) length(unique(x)) == 1))) {
warning("Non-informative exogenous variables, using simple ARIMA")
model <- auto.arima(ts_data)
} else {
# Fit ARIMAX model
tryCatch({
model <- auto.arima(ts_data, xreg = exog_matrix)
}, error = function(e) {
warning(paste("Error fitting ARIMAX:", e$message, "\nUsing simple ARIMA"))
model <- auto.arima(ts_data)
})
}
}
}
} else {
# Use simple ARIMA if no significant exogenous variables
cat(sprintf("No significant exogenous variables for %s, using simple ARIMA\n", current_ticker))
model <- auto.arima(ts_data)
}
return(model)
}
# Function to evaluate model performance
evaluate_model <- function(model, test_data, ticker) {
# Validate test data
if (nrow(test_data) == 0) {
stop("No test data available")
}
# Generate forecasts
if (inherits(model, "ARIMA")) {
forecast_values <- forecast(model, h = nrow(test_data))
} else {
exog_test <- test_data %>%
select(matches("pred_daily")) %>%
as.matrix()
# Adjust dimensions if needed
if (nrow(exog_test) != nrow(test_data)) {
warning("Dimension mismatch in test data")
h <- min(nrow(exog_test), nrow(test_data))
forecast_values <- forecast(model, xreg = exog_test[1:h,], h = h)
} else {
forecast_values <- forecast(model, xreg = exog_test, h = nrow(test_data))
}
}
# Calculate RMSE
rmse <- sqrt(mean((test_data$returns - forecast_values$mean)^2))
return(list(
ticker = ticker,
rmse = rmse,
forecast = forecast_values$mean
))
}
# Prepare data for modeling
splits_list <- df_combined %>%
group_by(ticker) %>%
group_map(~ {
.x <- .x %>% arrange(date)
time_series_split(
.x,
assess = "3 months",
cumulative = TRUE
)
})
# Initialize result storage
local_models <- list()
local_forecasts <- list()
local_rmse <- list()
train_forecasts <- list()
# Fit models for each ticker
for (i in seq_along(unique(df_combined$ticker))) {
ticker <- unique(df_combined$ticker)[i]
cat(sprintf("\nProcessing %s...\n", ticker))
# Extract ticker data
ticker_data <- df_combined %>%
filter(ticker == !!ticker) %>%
arrange(date)
# Check data sufficiency
if (nrow(ticker_data) < 50) {
cat(sprintf("Insufficient data for %s, skipping...\n", ticker))
next
}
tryCatch({
# Fit local model
local_model <- fit_local_model(
ticker_data = ticker_data,
granger_results = df_results_sorted,
prediction_markets = df_pred_all
)
# Evaluate model
evaluation <- evaluate_model(
model = local_model,
test_data = testing(splits_list[[i]]),
ticker = ticker
)
# Generate train set forecasts
train_data <- training(splits_list[[i]])
if (inherits(local_model, "ARIMA")) {
train_forecast <- fitted(local_model)
} else {
exog_train <- train_data %>%
select(matches("pred_daily")) %>%
as.matrix()
train_forecast <- fitted(local_model)
}
# Store results
local_models[[ticker]] <- local_model
local_forecasts[[ticker]] <- evaluation$forecast
local_rmse[[ticker]] <- evaluation$rmse
train_forecasts[[ticker]] <- train_forecast
cat(sprintf("Model successfully fitted for %s\n", ticker))
}, error = function(e) {
cat(sprintf("Error processing %s: %s\n", ticker, e$message))
})
}
# Create performance summary
performance_summary <- data.frame(
ticker = names(local_rmse),
local_rmse = unlist(local_rmse)
)
################################################################################
# 12. ERROR ANALYSIS BY TIME POINT
################################################################################
# Create forecast error data frame
local_forecast_errors <- data.frame()
# Calculate forecast errors for each ticker
for (ticker in names(local_forecasts)) {
# Get test data
test_data <- testing(splits_list[[which(unique(df_combined$ticker) == ticker)]])
# Ensure forecast length matches test data
forecast_values <- local_forecasts[[ticker]]
if (length(forecast_values) > nrow(test_data)) {
forecast_values <- forecast_values[1:nrow(test_data)]
}
# Create error data frame
ticker_errors <- data.frame(
date = test_data$date[1:length(forecast_values)],
ticker = ticker,
y_t = test_data$returns[1:length(forecast_values)],
y_hat_t = forecast_values
)
# Calculate forecast error
ticker_errors <- ticker_errors %>%
mutate(e1_t = y_t - y_hat_t) %>%
filter(!is.na(y_t) & !is.na(y_hat_t))
# Add to main data frame
local_forecast_errors <- bind_rows(local_forecast_errors, ticker_errors)
}
# Remove duplicates
local_forecast_errors <- local_forecast_errors %>%
distinct(date, ticker, .keep_all = TRUE)
# Display first few forecast errors
cat("\nFirst few local model forecast errors:\n")
print(head(local_forecast_errors))
# Save results
saveRDS(local_forecast_errors, "data/rds/local_forecast_errors.rds")
saveRDS(local_models, "data/rds/local_models.rds")
saveRDS(local_forecasts, "data/rds/local_forecasts.rds")
saveRDS(performance_summary, "data/rds/local_performance_summary.rds")
################################################################################
# 13. VISUALIZATION FUNCTIONS
################################################################################
# Function to plot training results
plot_train_results <- function(ticker) {
train_data <- training(splits_list[[which(unique(df_combined$ticker) == ticker)]])
test_data <- testing(splits_list[[which(unique(df_combined$ticker) == ticker)]])
# Create training forecast data frame
train_forecast_df <- data.frame(
date = train_data$date[1:length(train_forecasts[[ticker]])],
forecast = train_forecasts[[ticker]]
)
# Calculate y-axis limits
y_min <- min(c(train_data$returns, test_data$returns, train_forecast_df$forecast), na.rm = TRUE)
y_max <- max(c(train_data$returns, test_data$returns, train_forecast_df$forecast), na.rm = TRUE)
# Create plot
p <- ggplot() +
geom_line(data = train_data, aes(x = date, y = returns, color = "Actual (train)"), linewidth = 0.8) +
geom_line(data = train_forecast_df,
aes(x = date, y = forecast, color = "Forecast (train)"), linewidth = 0.8) +
geom_line(data = test_data, aes(x = date, y = returns, color = "Actual (test)"), linewidth = 0.8) +
labs(title = paste("Model Results for", ticker),
x = "Date",
y = "Returns",
color = "Legend") +
theme_minimal() +
theme(legend.position = "bottom") +
ylim(y_min, y_max)
return(p)
}
# Function to plot future forecasts
plot_future_forecasts <- function(ticker) {
train_data <- training(splits_list[[which(unique(df_combined$ticker) == ticker)]])
test_data <- testing(splits_list[[which(unique(df_combined$ticker) == ticker)]])
# Generate future dates and forecasts
future_dates <- seq(max(test_data$date), by = "day", length.out = 60)
if (inherits(local_models[[ticker]], "ARIMA")) {
future_forecast <- forecast(local_models[[ticker]], h = 60)
} else {
last_exog <- test_data %>%
select(matches("pred_daily")) %>%
tail(1) %>%
as.matrix()
future_forecast <- forecast(local_models[[ticker]],
xreg = matrix(rep(last_exog, 60), ncol = ncol(last_exog), byrow = TRUE),
h = 60)
}
# Calculate y-axis limits
y_min <- min(c(train_data$returns, test_data$returns, future_forecast$mean), na.rm = TRUE)
y_max <- max(c(train_data$returns, test_data$returns, future_forecast$mean), na.rm = TRUE)
# Create plot
p <- ggplot() +
geom_line(data = train_data, aes(x = date, y = returns, color = "Actual (train)"), linewidth = 0.8) +
geom_line(data = test_data, aes(x = date, y = returns, color = "Actual (test)"), linewidth = 0.8) +
geom_line(data = data.frame(date = future_dates, forecast = future_forecast$mean),
aes(x = date, y = forecast, color = "Future Forecast"), linewidth = 0.8) +
geom_ribbon(data = data.frame(date = future_dates,
lower = future_forecast$lower[,2],
upper = future_forecast$upper[,2]),
aes(x = date, ymin = lower, ymax = upper), alpha = 0.2) +
labs(title = paste("Future Forecasts for", ticker),
x = "Date",
y = "Returns",
color = "Legend") +
theme_minimal() +
theme(legend.position = "bottom") +
ylim(y_min, y_max)
return(p)
}
################################################################################
# 1. LIBRARY SETUP AND PACKAGE INSTALLATION
################################################################################
# Required libraries for model comparison and visualization
libraries <- c("forecast", "dplyr", "tidyr", "ggplot2", "Metrics", "modeltime", "knitr", "kableExtra")
# Install and load required packages
for (lib in libraries) {
if (!require(lib, character.only = TRUE)) {
install.packages(lib, dependencies = TRUE)
library(lib, character.only = TRUE)
}
}
################################################################################
# 2. LOAD SAVED MODELS AND RESULTS
################################################################################
# Load RMSE results for both models
global_rmse <- readRDS("data/rds/global_model_rmse.rds")
local_rmse <- readRDS("data/rds/local_performance_summary.rds")
# Load forecast results for error calculations
global_forecasts <- readRDS("data/rds/global_model_forecasts.rds")
local_forecasts <- readRDS("data/rds/local_forecasts.rds")
# Load forecast errors for both models
global_errors <- readRDS("data/rds/forecast_errors.rds")
local_errors <- readRDS("data/rds/local_forecast_errors.rds")
################################################################################
# 3. RMSE COMPARISON
################################################################################
# Create comparison dataframe by joining global and local RMSE results
comparison_df <- inner_join(
global_rmse,
local_rmse %>% select(ticker, local_rmse = local_rmse),
by = "ticker"
)
# Calculate differences and determine better model
comparison_df <- comparison_df %>%
mutate(
rmse_difference = local_rmse - global_rmse,
percent_difference = (rmse_difference / global_rmse) * 100,
better_model = case_when(
local_rmse < global_rmse ~ "Local",
local_rmse > global_rmse ~ "Global",
TRUE ~ "Equal"
)
)
# Create formatted comparison table
comparison_table <- comparison_df %>%
arrange(percent_difference) %>%
select(
Ticker = ticker,
`Global RMSE` = global_rmse,
`Local RMSE` = local_rmse,
`RMSE Difference` = rmse_difference,
`% Difference` = percent_difference,
`Better Model` = better_model
) %>%
mutate(
`Global RMSE` = round(`Global RMSE`, 4),
`Local RMSE` = round(`Local RMSE`, 4),
`RMSE Difference` = round(`RMSE Difference`, 4),
`% Difference` = round(`% Difference`, 2)
)
# Display and save comparison table
print(kable(comparison_table, format = "html", caption = "RMSE Comparison by Ticker") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")))
save_kable(comparison_table, "data/rmse_comparison_table.html")
################################################################################
# 4. DIEBOLD-MARIANO TEST IMPLEMENTATION
################################################################################
# Function to perform Diebold-Mariano test for each ticker
perform_dm_test <- function(global_errors, local_errors, ticker) {
# Filter errors for specific ticker
global_ticker <- global_errors %>% filter(ticker == !!ticker)
local_ticker <- local_errors %>% filter(ticker == !!ticker)
# Convert dates to character strings for comparison
global_ticker <- global_ticker %>%
mutate(date_str = as.character(.index))
local_ticker <- local_ticker %>%
mutate(date_str = as.character(date))
# Find common dates between both models
common_dates <- intersect(global_ticker$date_str, local_ticker$date_str)
if (length(common_dates) < 2) {
return(data.frame(
ticker = ticker,
dm_statistic = NA,
p_value = NA,
conclusion = "Insufficient data",
n_observations = length(common_dates)
))
}
# Align errors on common dates
global_aligned <- global_ticker %>%
filter(date_str %in% common_dates) %>%
arrange(date_str) %>%
select(date_str, e1_t)
local_aligned <- local_ticker %>%
filter(date_str %in% common_dates) %>%
arrange(date_str) %>%
select(date_str, e1_t)
# Verify date alignment
if (!all(global_aligned$date_str == local_aligned$date_str)) {
return(data.frame(
ticker = ticker,
dm_statistic = NA,
p_value = NA,
conclusion = "Date alignment issue",
n_observations = nrow(global_aligned)
))
}
# Extract errors excluding NA values
global_errors <- global_aligned$e1_t[!is.na(global_aligned$e1_t)]
local_errors <- local_aligned$e1_t[!is.na(local_aligned$e1_t)]
# Calculate h parameter based on data size
n_obs <- length(global_errors)
h <- max(1, min(1, floor(n_obs / 2)))
if (n_obs < 2) {
return(data.frame(
ticker = ticker,
dm_statistic = NA,
p_value = NA,
conclusion = "Insufficient data",
n_observations = n_obs
))
}
# Perform Diebold-Mariano test
dm_result <- dm.test(global_errors, local_errors, alternative = "two.sided", h = h)
# Interpret results
conclusion <- case_when(
dm_result$p.value < 0.05 & dm_result$statistic > 0 ~ "Local model significantly better",
dm_result$p.value < 0.05 & dm_result$statistic < 0 ~ "Global model significantly better",
TRUE ~ "No significant difference"
)
return(data.frame(
ticker = ticker,
dm_statistic = dm_result$statistic,
p_value = dm_result$p.value,
conclusion = conclusion,
n_observations = n_obs
))
}
# Perform DM test for each ticker
dm_results <- data.frame()
for (ticker in unique(global_errors$ticker)) {
result <- perform_dm_test(global_errors, local_errors, ticker)
dm_results <- bind_rows(dm_results, result)
}
# Save DM test results
saveRDS(dm_results, "data/rds/dm_test_results.rds")
################################################################################
# 6. FINAL SUMMARY
################################################################################
# Create final summary combining RMSE and DM test results
final_summary <- comparison_df %>%
left_join(dm_results, by = "ticker") %>%
select(
Ticker = ticker,
`Global RMSE` = global_rmse,
`Local RMSE` = local_rmse,
`RMSE Difference` = rmse_difference,
`% Difference` = percent_difference,
`DM Statistic` = dm_statistic,
`DM p-value` = p_value,
`DM Conclusion` = conclusion
) %>%
mutate(
`Global RMSE` = round(`Global RMSE`, 4),
`Local RMSE` = round(`Local RMSE`, 4),
`RMSE Difference` = round(`RMSE Difference`, 4),
`% Difference` = round(`% Difference`, 2),
`DM Statistic` = round(`DM Statistic`, 4),
`DM p-value` = round(`DM p-value`, 4)
)
# Display and save final summary
print(kable(final_summary, format = "html", caption = "Final Model Comparison Summary") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")))
saveRDS(final_summary, "data/rds/final_comparison_summary.rds")
#save_kable(final_summary, "data/html/final_comparison_summary.html")
# Create comparison dataframe by joining global and local RMSE results
comparison_df <- inner_join(
global_rmse,
local_rmse %>% select(ticker, local_rmse = local_rmse),
by = "ticker"
)
# Calculate differences and determine better model
comparison_df <- comparison_df %>%
mutate(
rmse_difference = local_rmse - global_rmse,
percent_difference = (rmse_difference / global_rmse) * 100,
better_model = case_when(
local_rmse < global_rmse ~ "Local",
local_rmse > global_rmse ~ "Global",
TRUE ~ "Equal"
)
)
# Create formatted comparison table
comparison_table <- comparison_df %>%
arrange(percent_difference) %>%
select(
Ticker = ticker,
`Global RMSE` = global_rmse,
`Local RMSE` = local_rmse,
`RMSE Difference` = rmse_difference,
`% Difference` = percent_difference,
`Better Model` = better_model
) %>%
mutate(
`Global RMSE` = round(`Global RMSE`, 4),
`Local RMSE` = round(`Local RMSE`, 4),
`RMSE Difference` = round(`RMSE Difference`, 4),
`% Difference` = round(`% Difference`, 2)
)
# Display and save comparison table
print(kable(comparison_table, format = "html", caption = "RMSE Comparison by Ticker") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")))
save_kable(comparison_table, "data/rmse_comparison_table.html")
# Function to perform Diebold-Mariano test for each ticker
perform_dm_test <- function(global_errors, local_errors, ticker) {
# Filter errors for specific ticker
global_ticker <- global_errors %>% filter(ticker == !!ticker)
local_ticker <- local_errors %>% filter(ticker == !!ticker)
# Convert dates to character strings for comparison
global_ticker <- global_ticker %>%
mutate(date_str = as.character(.index))
local_ticker <- local_ticker %>%
mutate(date_str = as.character(date))
# Find common dates between both models
common_dates <- intersect(global_ticker$date_str, local_ticker$date_str)
if (length(common_dates) < 2) {
return(data.frame(
ticker = ticker,
dm_statistic = NA,
p_value = NA,
conclusion = "Insufficient data",
n_observations = length(common_dates)
))
}
# Align errors on common dates
global_aligned <- global_ticker %>%
filter(date_str %in% common_dates) %>%
arrange(date_str) %>%
select(date_str, e1_t)
local_aligned <- local_ticker %>%
filter(date_str %in% common_dates) %>%
arrange(date_str) %>%
select(date_str, e1_t)
# Verify date alignment
if (!all(global_aligned$date_str == local_aligned$date_str)) {
return(data.frame(
ticker = ticker,
dm_statistic = NA,
p_value = NA,
conclusion = "Date alignment issue",
n_observations = nrow(global_aligned)
))
}
# Extract errors excluding NA values
global_errors <- global_aligned$e1_t[!is.na(global_aligned$e1_t)]
local_errors <- local_aligned$e1_t[!is.na(local_aligned$e1_t)]
# Calculate h parameter based on data size
n_obs <- length(global_errors)
h <- max(1, min(1, floor(n_obs / 2)))
if (n_obs < 2) {
return(data.frame(
ticker = ticker,
dm_statistic = NA,
p_value = NA,
conclusion = "Insufficient data",
n_observations = n_obs
))
}
# Perform Diebold-Mariano test
dm_result <- dm.test(global_errors, local_errors, alternative = "two.sided", h = h)
# Interpret results
conclusion <- case_when(
dm_result$p.value < 0.05 & dm_result$statistic > 0 ~ "Local model significantly better",
dm_result$p.value < 0.05 & dm_result$statistic < 0 ~ "Global model significantly better",
TRUE ~ "No significant difference"
)
return(data.frame(
ticker = ticker,
dm_statistic = dm_result$statistic,
p_value = dm_result$p.value,
conclusion = conclusion,
n_observations = n_obs
))
}
