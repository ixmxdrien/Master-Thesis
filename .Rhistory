# Function to compute cross-correlation between two time series
compute_cross_correlation <- function(stock_series, pred_series, max_lag = 20) {
# Merge series on date
df_combined <- inner_join(stock_series, pred_series, by = "date", suffix = c("_stock", "_pred"))
df_clean <- df_combined %>% drop_na(pred_daily_stock, pred_daily_pred)
# Check data sufficiency
if (nrow(df_clean) < (max_lag + 1)) {
return(data.frame(
lag = NA,
correlation = NA,
stock = unique(stock_series$ticker)[1],
prediction_market = unique(pred_series$id)[1]
))
}
# Compute cross-correlation
ccf_result <- ccf(df_clean$pred_daily_stock, df_clean$pred_daily_pred,
lag.max = max_lag, plot = FALSE)
# Create result dataframe
result_df <- data.frame(
lag = ccf_result$lag,
correlation = ccf_result$acf,
stock = unique(stock_series$ticker)[1],
prediction_market = unique(pred_series$id)[1]
)
return(result_df)
}
# Prepare data for cross-correlation analysis
stock_series_list <- lapply(unique(df_stock$ticker), function(tk) {
df_stock %>% filter(ticker == tk) %>%
select(date, ticker, pred_daily = returns)
})
names(stock_series_list) <- unique(df_stock$ticker)
prediction_series_list <- lapply(unique(df_pred_all$id), function(id) {
df_pred_all %>% filter(id == !!id) %>%
select(date, id, pred_daily)
})
names(prediction_series_list) <- unique(df_pred_all$id)
# Compute cross-correlations for all combinations
cross_corr_results <- list()
for (stock_name in names(stock_series_list)) {
for (pred_name in names(prediction_series_list)) {
result <- compute_cross_correlation(
stock_series = stock_series_list[[stock_name]],
pred_series = prediction_series_list[[pred_name]]
)
cross_corr_results[[length(cross_corr_results) + 1]] <- result
}
}
# Combine all results
df_cross_corr <- bind_rows(cross_corr_results)
# Find significant correlations (using 95% confidence interval)
df_cross_corr_significant <- df_cross_corr %>%
filter(abs(correlation) > 1.96/sqrt(nrow(df_stock))) %>%
arrange(desc(abs(correlation)))
# Display top correlations
cat("\nTop 10 strongest cross-correlations:\n")
print(head(df_cross_corr_significant, 10))
# Save results
write_csv(df_cross_corr_significant, "data/csv/cross_correlation_results.csv")
# Function to perform diagnostic tests on a time series
perform_diagnostic_tests <- function(series, series_name) {
# Create a list to store results
results <- list()
# 1. Test for Normality (Shapiro-Wilk test)
shapiro_test <- shapiro.test(series)
results$normality <- data.frame(
test = "Shapiro-Wilk",
p_value = shapiro_test$p.value,
is_normal = shapiro_test$p.value > 0.05
)
# 2. Test for Heteroskedasticity (Breusch-Pagan test)
# Create a simple linear model for the test
lm_model <- lm(series ~ seq_along(series))
bp_test <- bptest(lm_model)
results$heteroskedasticity <- data.frame(
test = "Breusch-Pagan",
p_value = bp_test$p.value,
is_homoskedastic = bp_test$p.value > 0.05
)
# 3. Test for Structural Breaks (Chow test)
# Split the series in half
n <- length(series)
mid_point <- floor(n/2)
chow_test <- sctest(series ~ seq_along(series), type = "Chow", point = mid_point)
results$structural_break <- data.frame(
test = "Chow",
p_value = chow_test$p.value,
has_break = chow_test$p.value < 0.05
)
# 4. Outlier Detection (using IQR method)
q1 <- quantile(series, 0.25)
q3 <- quantile(series, 0.75)
iqr <- q3 - q1
lower_bound <- q1 - 1.5 * iqr
upper_bound <- q3 + 1.5 * iqr
outliers <- sum(series < lower_bound | series > upper_bound)
results$outliers <- data.frame(
test = "IQR",
n_outliers = outliers,
percentage = (outliers / length(series)) * 100
)
# Add series name to all results
for (i in seq_along(results)) {
results[[i]]$series_name <- series_name
}
return(results)
}
# Perform diagnostic tests on stock returns
stock_diagnostics <- list()
for (ticker in unique(df_stock$ticker)) {
stock_data <- df_stock %>%
filter(ticker == !!ticker) %>%
pull(returns)
stock_diagnostics[[ticker]] <- perform_diagnostic_tests(stock_data, ticker)
}
# Perform diagnostic tests on prediction market data
pred_diagnostics <- list()
for (pred_market in unique(df_pred_all$id)) {
pred_data <- df_pred_all %>%
filter(id == !!pred_market) %>%
pull(pred_daily)
pred_diagnostics[[pred_market]] <- perform_diagnostic_tests(pred_data, pred_market)
}
# Combine results
combine_diagnostic_results <- function(diagnostics_list) {
all_results <- list()
for (i in seq_along(diagnostics_list)) {
for (j in seq_along(diagnostics_list[[i]])) {
all_results[[length(all_results) + 1]] <- diagnostics_list[[i]][[j]]
}
}
return(bind_rows(all_results))
}
# Combine and save results
stock_diagnostic_results <- combine_diagnostic_results(stock_diagnostics)
pred_diagnostic_results <- combine_diagnostic_results(pred_diagnostics)
# Save results
write_csv(stock_diagnostic_results, "data/csv/stock_diagnostic_results.csv")
write_csv(pred_diagnostic_results, "data/csv/prediction_market_diagnostic_results.csv")
# Display summary of results
cat("\nSummary of Diagnostic Tests for Stocks:\n")
print(summary(stock_diagnostic_results))
cat("\nSummary of Diagnostic Tests for Prediction Markets:\n")
print(summary(pred_diagnostic_results))
# Function to perform Granger causality test
granger_causality_test <- function(pred_series, stock_series, max_lag = 5, pred_name = "Prediction Market", stock_name = "Stock") {
# Merge series on date
df_combined <- inner_join(pred_series, stock_series, by = "date", suffix = c("_pred", "_stock"))
df_clean <- df_combined %>% drop_na(pred_daily_pred, pred_daily_stock)
# Check data sufficiency
if (nrow(df_clean) < (max_lag + 1)) {
return(data.frame(
Prediction_Market = pred_name,
Stock = stock_name,
p_value = NA,
result = "Not enough data"
))
}
# Create time series
ts_pred <- df_clean$pred_daily_pred
ts_stock <- df_clean$pred_daily_stock
# Perform Granger test
test_result <- tryCatch({
grangertest(ts_stock ~ ts_pred, order = max_lag, data = df_clean)
}, error = function(e) {
return(NULL)
})
if (is.null(test_result)) {
return(data.frame(
Prediction_Market = pred_name,
Stock = stock_name,
p_value = NA,
result = "Test failed"
))
}
# Extract and interpret results
p_value <- test_result$`Pr(>F)`[2]
result <- ifelse(p_value < 0.05, "Granger-causes", "Does not Granger-cause")
return(data.frame(
Prediction_Market = pred_name,
Stock = stock_name,
p_value = p_value,
result = result
))
}
# Prepare data for Granger causality tests
prediction_markets <- list(
df_pred_daily_TESLA, df_pred_daily_NETFLIX, df_pred_daily_META, df_pred_daily_GDP,
df_pred_daily_SpaceX, df_pred_daily_gas_us, df_pred_daily_wti_oil, df_pred_daily_btc,
df_pred_daily_us_sc, df_pred_daily_infla, df_pred_daily_huricane, df_pred_daily_eth,
df_pred_daily_measles, df_pred_daily_apple
)
prediction_names <- sapply(prediction_markets, function(df) unique(df$id)[1])
stock_tickers <- unique(df_stock$ticker)
# Create stock series list
stock_series_list <- lapply(stock_tickers, function(tk) {
df_stock %>% filter(ticker == tk) %>%
select(date, pred_daily = returns)
})
names(stock_series_list) <- stock_tickers
# Perform Granger causality tests
results <- list()
for (i in seq_along(prediction_markets)) {
for (j in seq_along(stock_series_list)) {
result <- granger_causality_test(
pred_series = prediction_markets[[i]],
stock_series = stock_series_list[[j]],
max_lag = 5,
pred_name = prediction_names[i],
stock_name = names(stock_series_list)[j]
)
results[[length(results) + 1]] <- result
}
}
# Combine and sort results
df_results <- bind_rows(results)
df_results_sorted <- df_results %>% arrange(p_value)
# Create summary table for Granger causality
granger_summary <- df_results_sorted %>%
filter(!is.na(p_value) & p_value < 0.10) %>%  # Keep results up to 10% significance
select(
`Pred Market` = Prediction_Market,
Stock = Stock,
`P-value` = p_value,
Result = result
) %>%
mutate(
`P-value` = round(`P-value`, 4),
Result = ifelse(Result == "Granger-causes", "Granger-causes", "Does not Granger-cause")
) %>%
arrange(`P-value`)
# Display Granger causality summary table
print(kable(granger_summary,
format = "html",
caption = "Granger Causality Analysis Summary (p < 0.10)") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
row_spec(which(granger_summary$Result == "Granger-causes"), background = "#e6ffe6") %>%
row_spec(which(granger_summary$Result == "Does not Granger-cause"), background = "#ffe6e6"))
# Save the summary table
save_kable(granger_summary, "data/granger_causality_summary.html")
# Required libraries for model comparison and visualization
libraries <- c("forecast", "dplyr", "tidyr", "ggplot2", "Metrics", "modeltime", "knitr", "kableExtra")
# Install and load required packages
for (lib in libraries) {
if (!require(lib, character.only = TRUE)) {
install.packages(lib, dependencies = TRUE)
library(lib, character.only = TRUE)
}
}
# Load RMSE results for both models
global_rmse <- readRDS("data/rds/global_model_rmse.rds")
local_rmse <- readRDS("data/rds/local_performance_summary.rds")
# Print structure of RMSE data
cat("\nStructure of global_rmse:\n")
str(global_rmse)
print(head(global_rmse))
cat("\nStructure of local_rmse:\n")
str(local_rmse)
print(head(local_rmse))
# Load forecast errors for both models
global_errors <- readRDS("data/rds/forecast_errors.rds")
local_errors <- readRDS("data/rds/local_forecast_errors.rds")
# Print structure of error data
cat("\nStructure of global_errors:\n")
str(global_errors)
print(head(global_errors))
cat("\nStructure of local_errors:\n")
str(local_errors)
print(head(local_errors))
# Create comparison dataframe by joining global and local RMSE results
comparison_df <- inner_join(
global_rmse,
local_rmse,
by = "ticker"
)
# Create a more intuitive comparison table
comparison_table <- comparison_df %>%
mutate(
# Calculate differences for each fold
fold1_diff = local_rmse_fold1 - global_rmse_fold1,
fold2_diff = local_rmse_fold2 - global_rmse_fold2,
# Determine better model for each fold
fold1_better = case_when(
fold1_diff < 0 ~ "Local",
fold1_diff > 0 ~ "Global",
TRUE ~ "Equal"
),
fold2_better = case_when(
fold2_diff < 0 ~ "Local",
fold2_diff > 0 ~ "Global",
TRUE ~ "Equal"
),
# Calculate percentage improvements
fold1_improvement = (fold1_diff / global_rmse_fold1) * 100,
fold2_improvement = (fold2_diff / global_rmse_fold2) * 100
) %>%
select(
Ticker = ticker,
# Fold 1 results
`Global RMSE (Fold 1)` = global_rmse_fold1,
`Local RMSE (Fold 1)` = local_rmse_fold1,
`Fold 1 Diff` = fold1_diff,
`Fold 1 % Imp` = fold1_improvement,
`Fold 1 Better` = fold1_better,
# Fold 2 results
`Global RMSE (Fold 2)` = global_rmse_fold2,
`Local RMSE (Fold 2)` = local_rmse_fold2,
`Fold 2 Diff` = fold2_diff,
`Fold 2 % Imp` = fold2_improvement,
`Fold 2 Better` = fold2_better
) %>%
mutate(
# Round numeric columns
across(where(is.numeric), ~round(., 4)),
# Format percentage columns
`Fold 1 % Imp` = round(`Fold 1 % Imp`, 2),
`Fold 2 % Imp` = round(`Fold 2 % Imp`, 2)
)
# Display comparison table with conditional formatting
print(kable(comparison_table, format = "html", caption = "Model Comparison by Fold") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
row_spec(which(comparison_table$`Fold 1 Better` == "Local"), background = "#e6ffe6") %>%
row_spec(which(comparison_table$`Fold 1 Better` == "Global"), background = "#ffe6e6") %>%
row_spec(which(comparison_table$`Fold 2 Better` == "Local"), background = "#e6ffe6") %>%
row_spec(which(comparison_table$`Fold 2 Better` == "Global"), background = "#ffe6e6"))
# Save comparison table
save_kable(comparison_table, "data/rmse_comparison_table_wf.html")
# Print summary statistics
cat("\nSummary of Model Performance:\n")
cat("\nFold 1 Summary:\n")
fold1_summary <- comparison_df %>%
summarise(
local_better = sum(local_rmse_fold1 < global_rmse_fold1),
global_better = sum(local_rmse_fold1 > global_rmse_fold1),
equal = sum(local_rmse_fold1 == global_rmse_fold1),
avg_improvement = mean((local_rmse_fold1 - global_rmse_fold1) / global_rmse_fold1 * 100)
)
print(fold1_summary)
cat("\nFold 2 Summary:\n")
fold2_summary <- comparison_df %>%
summarise(
local_better = sum(local_rmse_fold2 < global_rmse_fold2),
global_better = sum(local_rmse_fold2 > global_rmse_fold2),
equal = sum(local_rmse_fold2 == global_rmse_fold2),
avg_improvement = mean((local_rmse_fold2 - global_rmse_fold2) / global_rmse_fold2 * 100)
)
print(fold2_summary)
# Create separate comparison tables for Fold 1 and Fold 2
fold1_table <- comparison_df %>%
mutate(
`Better Model` = case_when(
local_rmse_fold1 < global_rmse_fold1 ~ "Local",
local_rmse_fold1 > global_rmse_fold1 ~ "Global",
TRUE ~ "Equal"
)
) %>%
select(
Ticker = ticker,
`Global RMSE` = global_rmse_fold1,
`Local RMSE` = local_rmse_fold1,
`Better Model`
) %>%
mutate(
across(where(is.numeric), ~round(., 4))
)
fold2_table <- comparison_df %>%
mutate(
`Better Model` = case_when(
local_rmse_fold2 < global_rmse_fold2 ~ "Local",
local_rmse_fold2 > global_rmse_fold2 ~ "Global",
TRUE ~ "Equal"
)
) %>%
select(
Ticker = ticker,
`Global RMSE` = global_rmse_fold2,
`Local RMSE` = local_rmse_fold2,
`Better Model`
) %>%
mutate(
across(where(is.numeric), ~round(., 4))
)
# Display Fold 1 comparison table
print(kable(fold1_table, format = "html", caption = "Model Comparison - Fold 1") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
row_spec(which(fold1_table$`Better Model` == "Local"), background = "#e6ffe6") %>%
row_spec(which(fold1_table$`Better Model` == "Global"), background = "#ffe6e6"))
# Display Fold 2 comparison table
print(kable(fold2_table, format = "html", caption = "Model Comparison - Fold 2") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
row_spec(which(fold2_table$`Better Model` == "Local"), background = "#e6ffe6") %>%
row_spec(which(fold2_table$`Better Model` == "Global"), background = "#ffe6e6"))
# Display Fold 1 comparison table
print(kable(fold1_table, format = "html", caption = "Model Comparison - Fold 1") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
row_spec(which(fold1_table$`Better Model` == "Local"), background = "#e6ffe6") %>%
row_spec(which(fold1_table$`Better Model` == "Global"), background = "#ffe6e6"))
# Display Fold 2 comparison table
print(kable(fold2_table, format = "html", caption = "Model Comparison - Fold 2") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
row_spec(which(fold2_table$`Better Model` == "Local"), background = "#e6ffe6") %>%
row_spec(which(fold2_table$`Better Model` == "Global"), background = "#ffe6e6"))
# Function to perform Diebold-Mariano test for each ticker and fold
perform_dm_test <- function(global_errors, local_errors, ticker, fold) {
# Filter errors for specific ticker and fold
global_ticker <- global_errors %>% filter(ticker == !!ticker)
local_ticker <- local_errors %>% filter(ticker == !!ticker, fold == !!fold)
# Convert dates to character strings for comparison
global_ticker <- global_ticker %>%
mutate(date_str = as.character(.index))
local_ticker <- local_ticker %>%
mutate(date_str = as.character(date))
# Find common dates between both models
common_dates <- intersect(global_ticker$date_str, local_ticker$date_str)
if (length(common_dates) < 2) {
return(data.frame(
ticker = ticker,
fold = fold,
dm_statistic = NA,
p_value = NA,
conclusion = "Insufficient data",
n_observations = length(common_dates)
))
}
# Align errors on common dates
global_aligned <- global_ticker %>%
filter(date_str %in% common_dates) %>%
arrange(date_str) %>%
select(date_str, e1_t)
local_aligned <- local_ticker %>%
filter(date_str %in% common_dates) %>%
arrange(date_str) %>%
select(date_str, e1_t)
# Verify date alignment
if (!all(global_aligned$date_str == local_aligned$date_str)) {
return(data.frame(
ticker = ticker,
fold = fold,
dm_statistic = NA,
p_value = NA,
conclusion = "Date alignment issue",
n_observations = nrow(global_aligned)
))
}
# Extract errors excluding NA values
global_errors <- global_aligned$e1_t[!is.na(global_aligned$e1_t)]
local_errors <- local_aligned$e1_t[!is.na(local_aligned$e1_t)]
# Calculate h parameter based on data size
n_obs <- length(global_errors)
h <- max(1, min(1, floor(n_obs / 2)))
if (n_obs < 2) {
return(data.frame(
ticker = ticker,
fold = fold,
dm_statistic = NA,
p_value = NA,
conclusion = "Insufficient data",
n_observations = n_obs
))
}
# Perform Diebold-Mariano test
dm_result <- dm.test(global_errors, local_errors, alternative = "two.sided", h = h)
# Interpret results
conclusion <- case_when(
dm_result$p.value < 0.05 & dm_result$statistic > 0 ~ "Local model significantly better",
dm_result$p.value < 0.05 & dm_result$statistic < 0 ~ "Global model significantly better",
TRUE ~ "No significant difference"
)
return(data.frame(
ticker = ticker,
fold = fold,
dm_statistic = dm_result$statistic,
p_value = dm_result$p.value,
conclusion = conclusion,
n_observations = n_obs
))
}
# Perform DM test for each ticker and fold
dm_results <- data.frame()
for (ticker in unique(global_errors$ticker)) {
for (fold in c("Fold 1", "Fold 2")) {
result <- perform_dm_test(global_errors, local_errors, ticker, fold)
dm_results <- bind_rows(dm_results, result)
}
}
# Format DM results for display
dm_table <- dm_results %>%
mutate(
# Round numeric values
dm_statistic = round(dm_statistic, 4),
p_value = round(p_value, 4),
# Format conclusion with color indicators
conclusion = case_when(
conclusion == "Local model significantly better" ~ "Local ✓",
conclusion == "Global model significantly better" ~ "Global ✓",
conclusion == "No significant difference" ~ "No difference",
TRUE ~ conclusion
)
) %>%
select(
Ticker = ticker,
Fold = fold,
`DM Statistic` = dm_statistic,
`P-value` = p_value,
`Conclusion` = conclusion,
`N Observations` = n_observations
)
# Display DM results table with conditional formatting
print(kable(dm_table, format = "html", caption = "Diebold-Mariano Test Results by Fold") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
row_spec(which(dm_table$Conclusion == "Local ✓"), background = "#e6ffe6") %>%
row_spec(which(dm_table$Conclusion == "Global ✓"), background = "#ffe6e6"))
