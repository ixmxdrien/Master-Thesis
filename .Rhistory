cross_corr_results <- list()
for (stock_name in names(stock_series_list)) {
for (pred_name in names(prediction_series_list)) {
result <- compute_cross_correlation(
stock_series = stock_series_list[[stock_name]],
pred_series = prediction_series_list[[pred_name]]
)
cross_corr_results[[length(cross_corr_results) + 1]] <- result
}
}
# Combine all results
df_cross_corr <- bind_rows(cross_corr_results)
# Find significant correlations (using 95% confidence interval)
df_cross_corr_significant <- df_cross_corr %>%
filter(abs(correlation) > 1.96/sqrt(nrow(df_stock))) %>%
arrange(desc(abs(correlation)))
# Display top correlations
cat("\nTop 10 strongest cross-correlations:\n")
print(head(df_cross_corr_significant, 10))
# Save results
write_csv(df_cross_corr_significant, "data/csv/cross_correlation_results.csv")
# Function to perform diagnostic tests on a time series
perform_diagnostic_tests <- function(series, series_name) {
# Create a list to store results
results <- list()
# 1. Test for Normality (Shapiro-Wilk test)
shapiro_test <- shapiro.test(series)
results$normality <- data.frame(
test = "Shapiro-Wilk",
p_value = shapiro_test$p.value,
is_normal = shapiro_test$p.value > 0.05
)
# 2. Test for Heteroskedasticity (Breusch-Pagan test)
# Create a simple linear model for the test
lm_model <- lm(series ~ seq_along(series))
bp_test <- bptest(lm_model)
results$heteroskedasticity <- data.frame(
test = "Breusch-Pagan",
p_value = bp_test$p.value,
is_homoskedastic = bp_test$p.value > 0.05
)
# 3. Test for Structural Breaks (Chow test)
# Split the series in half
n <- length(series)
mid_point <- floor(n/2)
chow_test <- sctest(series ~ seq_along(series), type = "Chow", point = mid_point)
results$structural_break <- data.frame(
test = "Chow",
p_value = chow_test$p.value,
has_break = chow_test$p.value < 0.05
)
# 4. Outlier Detection (using IQR method)
q1 <- quantile(series, 0.25)
q3 <- quantile(series, 0.75)
iqr <- q3 - q1
lower_bound <- q1 - 1.5 * iqr
upper_bound <- q3 + 1.5 * iqr
outliers <- sum(series < lower_bound | series > upper_bound)
results$outliers <- data.frame(
test = "IQR",
n_outliers = outliers,
percentage = (outliers / length(series)) * 100
)
# Add series name to all results
for (i in seq_along(results)) {
results[[i]]$series_name <- series_name
}
return(results)
}
# Perform diagnostic tests on stock returns
stock_diagnostics <- list()
for (ticker in unique(df_stock$ticker)) {
stock_data <- df_stock %>%
filter(ticker == !!ticker) %>%
pull(returns)
stock_diagnostics[[ticker]] <- perform_diagnostic_tests(stock_data, ticker)
}
# Perform diagnostic tests on prediction market data
pred_diagnostics <- list()
for (pred_market in unique(df_pred_all$id)) {
pred_data <- df_pred_all %>%
filter(id == !!pred_market) %>%
pull(pred_daily)
pred_diagnostics[[pred_market]] <- perform_diagnostic_tests(pred_data, pred_market)
}
# Combine results
combine_diagnostic_results <- function(diagnostics_list) {
all_results <- list()
for (i in seq_along(diagnostics_list)) {
for (j in seq_along(diagnostics_list[[i]])) {
all_results[[length(all_results) + 1]] <- diagnostics_list[[i]][[j]]
}
}
return(bind_rows(all_results))
}
# Combine and save results
stock_diagnostic_results <- combine_diagnostic_results(stock_diagnostics)
pred_diagnostic_results <- combine_diagnostic_results(pred_diagnostics)
# Save results
write_csv(stock_diagnostic_results, "data/csv/stock_diagnostic_results.csv")
write_csv(pred_diagnostic_results, "data/csv/prediction_market_diagnostic_results.csv")
# Display summary of results
cat("\nSummary of Diagnostic Tests for Stocks:\n")
print(summary(stock_diagnostic_results))
cat("\nSummary of Diagnostic Tests for Prediction Markets:\n")
print(summary(pred_diagnostic_results))
# Function to perform Granger causality test
granger_causality_test <- function(pred_series, stock_series, max_lag = 5, pred_name = "Prediction Market", stock_name = "Stock") {
# Merge series on date
df_combined <- inner_join(pred_series, stock_series, by = "date", suffix = c("_pred", "_stock"))
df_clean <- df_combined %>% drop_na(pred_daily_pred, pred_daily_stock)
# Check data sufficiency
if (nrow(df_clean) < (max_lag + 1)) {
return(data.frame(
Prediction_Market = pred_name,
Stock = stock_name,
p_value = NA,
result = "Not enough data"
))
}
# Create time series
ts_pred <- df_clean$pred_daily_pred
ts_stock <- df_clean$pred_daily_stock
# Perform Granger test
test_result <- tryCatch({
grangertest(ts_stock ~ ts_pred, order = max_lag, data = df_clean)
}, error = function(e) {
return(NULL)
})
if (is.null(test_result)) {
return(data.frame(
Prediction_Market = pred_name,
Stock = stock_name,
p_value = NA,
result = "Test failed"
))
}
# Extract and interpret results
p_value <- test_result$`Pr(>F)`[2]
result <- ifelse(p_value < 0.05, "Granger-causes", "Does not Granger-cause")
return(data.frame(
Prediction_Market = pred_name,
Stock = stock_name,
p_value = p_value,
result = result
))
}
# Prepare data for Granger causality tests
prediction_markets <- list(
df_pred_daily_TESLA, df_pred_daily_NETFLIX, df_pred_daily_META, df_pred_daily_GDP,
df_pred_daily_SpaceX, df_pred_daily_gas_us, df_pred_daily_wti_oil, df_pred_daily_btc,
df_pred_daily_us_sc, df_pred_daily_infla, df_pred_daily_huricane, df_pred_daily_eth,
df_pred_daily_measles, df_pred_daily_apple
)
prediction_names <- sapply(prediction_markets, function(df) unique(df$id)[1])
stock_tickers <- unique(df_stock$ticker)
# Create stock series list
stock_series_list <- lapply(stock_tickers, function(tk) {
df_stock %>% filter(ticker == tk) %>%
select(date, pred_daily = returns)
})
names(stock_series_list) <- stock_tickers
# Perform Granger causality tests
results <- list()
for (i in seq_along(prediction_markets)) {
for (j in seq_along(stock_series_list)) {
result <- granger_causality_test(
pred_series = prediction_markets[[i]],
stock_series = stock_series_list[[j]],
max_lag = 5,
pred_name = prediction_names[i],
stock_name = names(stock_series_list)[j]
)
results[[length(results) + 1]] <- result
}
}
# Combine and sort results
df_results <- bind_rows(results)
df_results_sorted <- df_results %>% arrange(p_value)
# Create summary table for Granger causality
granger_summary <- df_results_sorted %>%
filter(!is.na(p_value) & p_value < 0.10) %>%  # Keep results up to 10% significance
select(
`Pred Market` = Prediction_Market,
Stock = Stock,
`P-value` = p_value,
Result = result
) %>%
mutate(
`P-value` = round(`P-value`, 4),
Result = ifelse(Result == "Granger-causes", "Granger-causes", "Does not Granger-cause")
) %>%
arrange(`P-value`)
# Display Granger causality summary table
print(kable(granger_summary,
format = "html",
caption = "Granger Causality Analysis Summary (p < 0.10)") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
row_spec(which(granger_summary$Result == "Granger-causes"), background = "#e6ffe6") %>%
row_spec(which(granger_summary$Result == "Does not Granger-cause"), background = "#ffe6e6"))
# Save the summary table
save_kable(granger_summary, "data/granger_causality_summary.html")
# Display significant results
print(df_results_sorted %>% filter(!is.na(p_value) & p_value < 0.15))
# Save results
write_csv(df_results_sorted, "data/csv/granger_results_all_combinations.csv")
# Combine predictions with stock data
df_combined <- df_pred_all %>%
left_join(df_stock, by = c("date")) %>%
filter(!is.na(returns))
View(df_combined)
# Process daily data for each dataset
process_df_daily <- function(df, ticker) {
df %>%
mutate(date = as.Date(Timestamp)) %>%
mutate(id = ticker) %>%
group_by(date, id) %>%
summarise(pred_daily = mean(Value, na.rm = TRUE))
}
# Process regular datasets
df_pred_daily_TESLA <- process_df_daily(df_Tesla, "TESLA")
df_pred_daily_NETFLIX <- process_df_daily(df_Netflix, "NETFLIX")
df_pred_daily_META <- process_df_daily(df_Meta, "META")
df_pred_daily_GDP <- process_df_daily(df_GDP, "GDP")
df_pred_daily_SpaceX <- process_df_daily(df_SpaceX, "SpaceX")
df_pred_daily_gas_us <- process_df_daily(df_gas_us, "gas_us")
df_pred_daily_wti_oil <- process_df_daily(df_wti_oil, "wti_oil")
df_pred_daily_btc <- process_df_daily(df_btc, "btc")
df_pred_daily_infla <- process_df_daily(df_infla, "infla")
df_pred_daily_huricane <- process_df_daily(df_huricane, "huricane")
df_pred_daily_eth <- process_df_daily(df_eth, "eth")
df_pred_daily_measles <- process_df_daily(df_measles, "measles")
# Process forecast data with percentage values
process_df_daily_forecast <- function(df, ticker) {
df %>%
mutate(
date = as.Date(Timestamp),
Forecast = as.numeric(gsub("%", "", Forecast)),
id = ticker
) %>%
group_by(date, id) %>%
summarise(pred_daily = mean(Forecast, na.rm = TRUE)) %>%
ungroup()
}
# Process forecast datasets
df_pred_daily_google <- process_df_daily_forecast(df_google_sp, "Google")
df_pred_daily_fed <- process_df_daily_forecast(df_fed_rate, "Fed Rate")
df_pred_daily_layoffs <- process_df_daily_forecast(df_layoffs, "Layoffs")
df_pred_daily_apple <- process_df_daily_forecast(df_apple, "Apple")
# Create complete date sequences for each dataset
complete_dates_TESLA <- tibble(date = seq(min(df_pred_daily_TESLA$date, na.rm = TRUE),
max(df_pred_daily_TESLA$date, na.rm = TRUE), by = "day"))
complete_dates_NETFLIX <- tibble(date = seq(min(df_pred_daily_NETFLIX$date, na.rm = TRUE),
max(df_pred_daily_NETFLIX$date, na.rm = TRUE), by = "day"))
complete_dates_META <- tibble(date = seq(min(df_pred_daily_META$date, na.rm = TRUE),
max(df_pred_daily_META$date, na.rm = TRUE), by = "day"))
complete_dates_GDP <- tibble(date = seq(min(df_pred_daily_GDP$date, na.rm = TRUE),
max(df_pred_daily_GDP$date, na.rm = TRUE), by = "day"))
# Join with original datasets to ensure complete date coverage
df_pred_daily_TESLA <- complete_dates_TESLA %>% left_join(df_pred_daily_TESLA, by = "date")
df_pred_daily_NETFLIX <- complete_dates_NETFLIX %>% left_join(df_pred_daily_NETFLIX, by = "date")
df_pred_daily_META <- complete_dates_META %>% left_join(df_pred_daily_META, by = "date")
df_pred_daily_GDP <- complete_dates_GDP %>% left_join(df_pred_daily_GDP, by = "date")
# Apply Kalman filter to fill missing values
apply_kalman_filter <- function(df) {
ts_data <- ts(df$pred_daily, frequency = 365, start = c(2024, 1))
model <- SSModel(ts_data ~ -1 + SSMtrend(degree = 1, Q = 1))
kalman_fit <- KFS(model, simplify = TRUE)
kalman_values <- kalman_fit$a
if (length(kalman_values) != nrow(df)) {
kalman_values <- kalman_values[1:nrow(df)]
}
df$pred_daily <- ifelse(is.na(df$pred_daily), kalman_values, df$pred_daily)
df$id[is.na(df$id)] <- unique(df$id[!is.na(df$id)])[1]
return(df)
}
# Apply Kalman filter to main datasets
df_pred_daily_TESLA <- apply_kalman_filter(df_pred_daily_TESLA)
df_pred_daily_NETFLIX <- apply_kalman_filter(df_pred_daily_NETFLIX)
df_pred_daily_META <- apply_kalman_filter(df_pred_daily_META)
df_pred_daily_GDP <- apply_kalman_filter(df_pred_daily_GDP)
# Load and process ETF data
df_etf <- read_csv("ETF/combined_returns_2024.csv") %>%
drop_na() %>%
filter(ticker != "CAC.PA")
# Load and process stock data
df_stock <- read_csv("data/stocks_and_pred/tilt_stocks_2024.csv") %>%
select(-`...1`) %>%
filter(ticker %in% c("TSLA", "NFLX", "META", "GOOG", "COIN", "INTC", "JPM", "XOM", "AAPL",
"BAC", "TOT", "PFE", "JNJ", "MSFT", "AMZN", "WMT", "NVDA")) %>%
mutate(date = as.Date(date))
# Combine stock and ETF data
df_stock <- bind_rows(df_stock, df_etf)
# Combine all prediction datasets
df_pred_all <- bind_rows(
df_pred_daily_TESLA, df_pred_daily_NETFLIX, df_pred_daily_META, df_pred_daily_GDP,
df_pred_daily_SpaceX, df_pred_daily_gas_us, df_pred_daily_wti_oil, df_pred_daily_btc,
df_pred_daily_infla, df_pred_daily_huricane, df_pred_daily_eth,
df_pred_daily_measles, df_pred_daily_apple
)
# Combine predictions with stock data
df_combined <- df_pred_all %>% left_join(df_stock, by = c("date"))
df_combined <- df_combined %>% filter(!is.na(returns))
# Create final data table for modeling
data_tbl <- df_combined %>% select(date, ticker, value = returns, id, pred_daily)
# Create time series split for modeling
data_tbl %>%
group_by(ticker) %>%
plot_time_series(
date, value, .interactive = FALSE, .facet_ncol = 3
)
# Create walk-forward time series split
splits <- data_tbl %>%
arrange(date) %>%  # Ensure data is ordered by date
time_series_cv(
date_var   = date,
initial    = "150 days",     # ~5 months
assess     = "75 days",      # ~2.5 months
skip       = "75 days",      # Step of 2.5 months
cumulative = TRUE,          # Keep all previous data in training
slice_limit = 2             # Limit to 2 folds
)
# Print splits to verify
print(splits)
# Get both splits for training and testing
first_split <- splits$splits[[1]]
second_split <- splits$splits[[2]]
# Get training and testing data for both splits
train_data_1 <- analysis(first_split)
test_data_1 <- assessment(first_split)
train_data_2 <- analysis(second_split)
test_data_2 <- assessment(second_split)
# Create XGBoost preprocessing recipe
rec_obj_xgb_1 <- recipe(value ~ ., data = train_data_1) %>%
step_dummy(id) %>%
step_normalize(all_numeric(), -all_outcomes()) %>%
step_timeseries_signature(date) %>%
step_rm(date) %>%
step_zv(all_predictors()) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE)
rec_obj_xgb_2 <- recipe(value ~ ., data = train_data_2) %>%
step_dummy(id) %>%
step_normalize(all_numeric(), -all_outcomes()) %>%
step_timeseries_signature(date) %>%
step_rm(date) %>%
step_zv(all_predictors()) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE)
# Create and fit XGBoost workflow for first fold
wflw_xgb_1 <- workflow() %>%
add_model(boost_tree("regression") %>% set_engine("xgboost")) %>%
add_recipe(rec_obj_xgb_1) %>%
fit(train_data_1)
# Create and fit XGBoost workflow for second fold
wflw_xgb_2 <- workflow() %>%
add_model(boost_tree("regression") %>% set_engine("xgboost")) %>%
add_recipe(rec_obj_xgb_2) %>%
fit(train_data_2)
# Create model table and calibrate for both folds
model_tbl_1 <- modeltime_table(wflw_xgb_1)
model_tbl_2 <- modeltime_table(wflw_xgb_2)
calib_tbl_1 <- model_tbl_1 %>%
modeltime_calibrate(
new_data = test_data_1,
id = "ticker"
)
calib_tbl_2 <- model_tbl_2 %>%
modeltime_calibrate(
new_data = test_data_2,
id = "ticker"
)
# Evaluate model accuracy for both folds
# Overall accuracy metrics
cat("\nAccuracy metrics for first fold:\n")
calib_tbl_1 %>%
modeltime_accuracy(acc_by_id = FALSE) %>%
table_modeltime_accuracy(.interactive = FALSE)
cat("\nAccuracy metrics for second fold:\n")
calib_tbl_2 %>%
modeltime_accuracy(acc_by_id = FALSE) %>%
table_modeltime_accuracy(.interactive = FALSE)
# Accuracy metrics by ticker
cat("\nAccuracy metrics by ticker for first fold:\n")
calib_tbl_1 %>%
modeltime_accuracy(acc_by_id = TRUE) %>%
table_modeltime_accuracy(.interactive = FALSE)
cat("\nAccuracy metrics by ticker for second fold:\n")
calib_tbl_2 %>%
modeltime_accuracy(acc_by_id = TRUE) %>%
table_modeltime_accuracy(.interactive = FALSE)
# Visualize model performance for both folds
cat("\nVisualizing first fold performance:\n")
calib_tbl_1 %>%
modeltime_forecast(
new_data = test_data_1,
actual_data = data_tbl,
conf_by_id = TRUE
) %>%
group_by(ticker) %>%
plot_modeltime_forecast(
.facet_ncol = 3,
.interactive = FALSE
)
View(df_eth)
calib_tbl_1 %>%
modeltime_forecast(
new_data = test_data_1,
actual_data = data_tbl,
conf_by_id = TRUE
) %>%
group_by(ticker) %>%
plot_modeltime_forecast(
.facet_ncol = 3,
.interactive = FALSE
)
cat("\nVisualizing second fold performance:\n")
calib_tbl_2 %>%
modeltime_forecast(
new_data = test_data_2,
actual_data = data_tbl,
conf_by_id = TRUE
) %>%
group_by(ticker) %>%
plot_modeltime_forecast(
.facet_ncol = 3,
.interactive = FALSE
)
# Calculate forecast errors for both folds
forecast_test_1 <- calib_tbl_1 %>%
modeltime_forecast(
new_data = test_data_1,
actual_data = data_tbl,
conf_by_id = TRUE
)
forecast_test_2 <- calib_tbl_2 %>%
modeltime_forecast(
new_data = test_data_2,
actual_data = data_tbl,
conf_by_id = TRUE
)
# Combine forecast errors from both folds
forecast_errors <- bind_rows(
# First fold errors
forecast_test_1 %>%
group_by(.index, ticker) %>%
mutate(
has_actual = any(.key == "actual"),
has_prediction = any(.key == "prediction")
) %>%
filter(has_actual & has_prediction) %>%
summarise(
y_t = mean(.value[.key == "actual"], na.rm = TRUE),
y_hat_t = mean(.value[.key == "prediction"], na.rm = TRUE),
fold = "Fold 1",
.groups = "drop"
) %>%
mutate(e1_t = y_t - y_hat_t),
# Second fold errors
forecast_test_2 %>%
group_by(.index, ticker) %>%
mutate(
has_actual = any(.key == "actual"),
has_prediction = any(.key == "prediction")
) %>%
filter(has_actual & has_prediction) %>%
summarise(
y_t = mean(.value[.key == "actual"], na.rm = TRUE),
y_hat_t = mean(.value[.key == "prediction"], na.rm = TRUE),
fold = "Fold 2",
.groups = "drop"
) %>%
mutate(e1_t = y_t - y_hat_t)
) %>%
filter(!is.na(y_t) & !is.na(y_hat_t))
# Display forecast errors
cat("\nForecast errors for both folds:\n")
print(head(forecast_errors))
# Save forecast errors
saveRDS(forecast_errors, "data/rds/forecast_errors.rds")
# Refit model on full dataset
refit_tbl <- calib_tbl_1 %>%
modeltime_refit(data = data_tbl)
# Prepare data for future predictions
data_tbl <- data_tbl %>%
distinct(ticker, date, .keep_all = TRUE)
# Create future time frame for predictions
future_tbl <- data_tbl %>%
group_by(ticker) %>%
future_frame(
.length_out = 30,  # Réduit à 30 jours au lieu de 60 car nous avons moins de données
.date_var = date,
.bind_data = FALSE
) %>%
mutate(
id = NA,
pred_daily = NA
)
# Generate and visualize forecasts
forecast_results <- refit_tbl %>%
modeltime_forecast(
new_data = future_tbl,
actual_data = data_tbl,
conf_by_id = TRUE
)
# Visualize forecasts with adjusted parameters
forecast_results %>%
group_by(ticker) %>%
plot_modeltime_forecast(
.interactive = FALSE,
.facet_ncol = 2,
.conf_interval_show = TRUE,  # Afficher les intervalles de confiance
.conf_interval_alpha = 0.1   # Ajuster la transparence des intervalles
)
